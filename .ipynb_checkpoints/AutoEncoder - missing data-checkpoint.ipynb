{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "# from tensorflow.keras.utils import np_utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# from tensorflow.keras.objectives import mse\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder:\n",
    "\n",
    "    def __init__(self, data,\n",
    "                 recurrent_weight=0.5,\n",
    "                 optimizer=\"adam\",\n",
    "                 dropout_probability=0.5,\n",
    "                 hidden_activation=\"relu\",\n",
    "                 output_activation=\"sigmoid\",\n",
    "                 init=\"glorot_normal\",\n",
    "                 l1_penalty=0,\n",
    "                 l2_penalty=0):\n",
    "        self.data = data.copy()\n",
    "        self.recurrent_weight = recurrent_weight\n",
    "        self.optimizer = optimizer\n",
    "        self.dropout_probability = dropout_probability\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.output_activation = output_activation\n",
    "        self.init = init\n",
    "        self.l1_penalty = l1_penalty\n",
    "        self.l2_penalty = l2_penalty\n",
    "        \n",
    "    def _get_hidden_layer_sizes(self):\n",
    "        n_dims = self.data.shape[1]\n",
    "        return [\n",
    "            min(2000, 8 * n_dims),\n",
    "            min(500, 2 * n_dims),\n",
    "            int(np.ceil(0.5 * n_dims)),\n",
    "        ]\n",
    "\n",
    "    def _create_model(self):\n",
    "\n",
    "        hidden_layer_sizes = self._get_hidden_layer_sizes()\n",
    "        first_layer_size = hidden_layer_sizes[0]\n",
    "        n_dims = self.data.shape[1]\n",
    "        \n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(\n",
    "            first_layer_size,\n",
    "            input_dim= 2 * n_dims,\n",
    "            activation=self.hidden_activation,\n",
    "            W_regularizer=L1L2(self.l1_penalty, self.l2_penalty),\n",
    "            init=self.init))\n",
    "        model.add(Dropout(self.dropout_probability))\n",
    "\n",
    "        for layer_size in hidden_layer_sizes[1:]:\n",
    "            model.add(Dense(\n",
    "                layer_size,\n",
    "                activation=self.hidden_activation,\n",
    "                W_regularizer=L1L2(self.l1_penalty, self.l2_penalty),\n",
    "                init=self.init))\n",
    "            model.add(Dropout(self.dropout_probability))\n",
    "\n",
    "        model.add(Dense(\n",
    "            n_dims,\n",
    "            activation=self.output_activation,\n",
    "            W_regularizer=L1L2(self.l1_penalty, self.l2_penalty),\n",
    "            init=self.init))\n",
    "\n",
    "        loss_function = make_reconstruction_loss(n_dims)\n",
    "\n",
    "        model.compile(optimizer=self.optimizer, loss=loss_function)\n",
    "        return model\n",
    "\n",
    "    def fill(self, missing_mask):\n",
    "        self.data[missing_mask] = -1\n",
    "\n",
    "    def _create_missing_mask(self):\n",
    "        if self.data.dtype != \"f\" and self.data.dtype != \"d\":\n",
    "            self.data = self.data.astype(float)\n",
    "\n",
    "        return np.isnan(self.data)\n",
    "\n",
    "    def _train_epoch(self, model, missing_mask, batch_size):\n",
    "        input_with_mask = np.hstack([self.data, missing_mask])\n",
    "        n_samples = len(input_with_mask)\n",
    "        n_batches = int(np.ceil(n_samples / batch_size))\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        X_shuffled = input_with_mask[indices]\n",
    "\n",
    "        for batch_idx in range(n_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx + 1) * batch_size\n",
    "            batch_data = X_shuffled[batch_start:batch_end, :]\n",
    "            model.train_on_batch(batch_data, batch_data)\n",
    "        return model.predict(input_with_mask)\n",
    "\n",
    "    def train(self, batch_size=256, train_epochs=100):\n",
    "        missing_mask = self._create_missing_mask()\n",
    "        self.fill(missing_mask)\n",
    "        self.model = self._create_model()\n",
    "\n",
    "        observed_mask = ~missing_mask\n",
    "\n",
    "        for epoch in range(train_epochs):\n",
    "            X_pred = self._train_epoch(self.model, missing_mask, batch_size)\n",
    "            observed_mae = masked_mae(X_true=self.data,\n",
    "                                    X_pred=X_pred,\n",
    "                                    mask=observed_mask)\n",
    "            if epoch % 50 == 0:\n",
    "                print(\"observed mae:\", observed_mae)\n",
    "\n",
    "            old_weight = (1.0 - self.recurrent_weight)\n",
    "            self.data[missing_mask] *= old_weight\n",
    "            pred_missing = X_pred[missing_mask]\n",
    "            self.data[missing_mask] += self.recurrent_weight * pred_missing\n",
    "        return self.data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/mushrooms.csv\")\n",
    "df = df.drop(['sroot'], axis=1)\n",
    "\n",
    "prob_missing = 0.1\n",
    "df_incomplete = df.copy()\n",
    "ix = [(row, col) for row in range(df.shape[0]) for col in range(df.shape[1])]\n",
    "for row, col in random.sample(ix, int(round(prob_missing * len(ix)))):\n",
    "    df_incomplete.iat[row, col] = np.nan\n",
    "    \n",
    "\n",
    "missing_encoded = pd.get_dummies(df_incomplete)\n",
    "\n",
    "for col in df.columns:\n",
    "    missing_cols = missing_encoded.columns.str.startswith(str(col) + \"_\")\n",
    "    missing_encoded.loc[df_incomplete[col].isnull(), missing_cols] = np.nan\n",
    "    \n",
    "imputer = Autoencoder(missing_encoded.values)\n",
    "complete_encoded = imputer.train(train_epochs=300, batch_size=256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
