{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/mag/scripts')\n",
    "\n",
    "from relationGraph import Relation, RelationGraph, MatrixOfRelationGraph\n",
    "from autoencoder import seedy, AutoEncoder\n",
    "import utilityFunctions as uf\n",
    "from main import test_build_relation_graph_with_symertic_data, test_convert_graph_to_2D_matrix, test_get_matix_for_autoencoder, test_autoencoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from base import load_source\n",
    "from base2 import load_dicty, load_pharma\n",
    "import utilityFunctions as uf\n",
    "import autoencoder as ae\n",
    "import multimodal as mm\n",
    "\n",
    "\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Layer, Reshape, UpSampling2D, Flatten, Masking, Dropout, concatenate, Conv2D, MaxPooling2D, BatchNormalization, Lambda\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "from tensorflow import set_random_seed\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "numOfExperiment = 3\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "0.0\n",
      "1.0\n",
      "-------------RelationGraph-------------\n",
      "Experimental condition\t282\n",
      "0\t\n",
      "1\texpr-(1219, 282)\n",
      "GO term\t116\n",
      "0\t\n",
      "1\tann-(1219, 116)\n",
      "Gene\t1219\n",
      "3\tann-(1219, 116), expr-(1219, 282), ppi-(1219, 1219)\n",
      "0\t\n",
      "\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "-------------RelationGraph-------------\n",
      "Chemical\t1260\n",
      "5\tactions-(1260, 130), pudmed-(1260, 7948), depositors-(1260, 189), fingerprints-(1260, 920), tanimoto-(1260, 1260)\n",
      "0\t\n",
      "Fingerprint\t920\n",
      "0\t\n",
      "1\tfingerprints-(1260, 920)\n",
      "Depositor category\t16\n",
      "0\t\n",
      "1\tdepo_cats-(189, 16)\n",
      "Depositor\t189\n",
      "1\tdepo_cats-(189, 16)\n",
      "1\tdepositors-(1260, 189)\n",
      "PMID\t7948\n",
      "0\t\n",
      "1\tpudmed-(1260, 7948)\n",
      "Action\t130\n",
      "0\t\n",
      "1\tactions-(1260, 130)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graph1 = load_dicty()\n",
    "graph2 = load_pharma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(filenames, n_pack):\n",
    "    # n_pack => 100 samples of matrix\n",
    "    # n_pack = batch_size\n",
    "    \n",
    "    files = []              # different files\n",
    "    num_packs = []          # subpacked inside of file\n",
    "    for filename in filenames:\n",
    "        f = np.load(filename)\n",
    "        files.append(f)\n",
    "        num_packs.append(f.files)\n",
    "    \n",
    "    counter = 0\n",
    "    while True:\n",
    "        x = []\n",
    "        for i in range(len(files)):\n",
    "            rand_num = np.random.randint(len(num_packs[i]))\n",
    "            f = files[i]\n",
    "            pack = num_packs[i]\n",
    "            x.append(f[pack[rand_num]])\n",
    "            \n",
    "        yield (x, x)\n",
    "        \n",
    "        if counter >= n_pack:\n",
    "            counter = 0\n",
    "    \n",
    "    \n",
    "def shuffle_data(org_data):\n",
    "    shufle_data = []\n",
    "    for data in org_data:\n",
    "        tmp_data = data\n",
    "        _, row, col, _ = tmp_data.shape\n",
    "        tmp_data = tmp_data.flatten()\n",
    "#         tmp_data[tmp_data > 0] = 1;   # set nonzero values to 1\n",
    "#         tmp_data = tmp_data * np.random.rand(len(tmp_data))  # multiply with random vecotor\n",
    "        np.random.shuffle(tmp_data)  # shuffle org data\n",
    "        shufle_data.append(np.array(tmp_data).reshape(1, row, col, 1))\n",
    "\n",
    "    return shufle_data\n",
    "\n",
    "\n",
    "def replace_and_shuffle_data_with_random(org_data):\n",
    "    shufle_data = []\n",
    "    for data in org_data:\n",
    "        tmp_data = data\n",
    "        _, row, col, _ = tmp_data.shape\n",
    "        tmp_data = tmp_data.flatten()\n",
    "        tmp_data[tmp_data > 0] = 1;   # set nonzero values to 1\n",
    "        tmp_data = tmp_data * np.random.rand(len(tmp_data))  # multiply with random vecotor\n",
    "#         np.random.shuffle(tmp_data)  # shuffle org data\n",
    "        shufle_data.append(np.array(tmp_data).reshape(1, row, col, 1))\n",
    "\n",
    "    return shufle_data\n",
    "\n",
    "    \n",
    "def order_inputs(model, org_data):\n",
    "    new_order_data = []\n",
    "    for inp in model.inputs:\n",
    "        _, x, y, _ = inp.shape\n",
    "        for data in org_data:\n",
    "            _, row, col, _ = data.shape\n",
    "            if x == row and y == col:\n",
    "                new_order_data.append(np.array(data).reshape(1,row,col,1))\n",
    "                break\n",
    "\n",
    "    return new_order_data\n",
    "            \n",
    "    \n",
    "class MultiModal:\n",
    "    \n",
    "    def __init__(self, graph=None, path=''):\n",
    "        self.input_visibles = []\n",
    "        self.input_layers = []\n",
    "        self.outputs_layers = []\n",
    "        self.input_data_size = []\n",
    "        self.filenames = []\n",
    "        self.inputs = None\n",
    "        self.model = None\n",
    "        self.path = path\n",
    "        self.org_data = []\n",
    "        self.predict_data = []\n",
    "        self.base_line = []\n",
    "        self.shuffled_data = []\n",
    "        \n",
    "        if graph is not None:\n",
    "            self._read_graph(graph)\n",
    "        self._callbacks()\n",
    "                    \n",
    "    def _read_graph(self, graph):\n",
    "        already = set()\n",
    "        for obj in graph.objects.values():        \n",
    "            for relation in obj.relation_x:\n",
    "                if relation.name not in already:\n",
    "                    self._set_params(relation)\n",
    "                    already.add(relation.name)\n",
    "\n",
    "            for relation in obj.relation_y:\n",
    "                if relation.name not in already:\n",
    "                    self._set_params(relation)\n",
    "                    already.add(relation.name)\n",
    "                        \n",
    "    def _set_params(self, relation):\n",
    "        print(relation.name + '\\t' + str(relation.matrix.shape))\n",
    "        self._input_layer(relation.matrix.shape)\n",
    "        \n",
    "        row, col = relation.matrix.shape\n",
    "        self.org_data.append(np.array(relation.matrix).reshape(1,row,col,1))\n",
    "        self.input_data_size.append(relation.matrix.shape)\n",
    "        self.filenames.append(self.path + relation.name + '.npz')\n",
    "        \n",
    "    def _input_layer(self, input_size):\n",
    "        row, col = input_size\n",
    "        visible = Input(shape=(row, col, 1))\n",
    "        \n",
    "#         Conv2D filtered by columns\n",
    "        layer1 = Conv2D(64, (1, col-7), activation='relu', padding='valid')(visible)\n",
    "        layer1 = MaxPooling2D((2, 2))(layer1)\n",
    "        layer1 = Conv2D(32, (3, 3), activation='relu')(layer1)\n",
    "        layer1 = MaxPooling2D((2, 2))(layer1)\n",
    "        layer1 = Conv2D(1, (1, 1), activation='relu')(layer1)\n",
    "        layer1 = Flatten()(layer1)\n",
    "        \n",
    "        layer2 = Conv2D(64, (1, col-7), activation='relu', padding='valid')(visible)\n",
    "        layer2 = MaxPooling2D((2, 2))(layer2)\n",
    "        layer2 = Conv2D(32, (3, 3), activation='relu')(layer2)\n",
    "        layer2 = MaxPooling2D((2, 2))(layer2)\n",
    "        layer2 = Conv2D(1, (1, 1), activation='relu')(layer2)\n",
    "        layer2 = Flatten()(layer2)\n",
    "        \n",
    "        layer = concatenate([layer1, layer2])\n",
    "        layer = Dense(int((row + col) / 2), activation='relu')(layer)\n",
    "        \n",
    "        layer = Dense(1000, activation=\"relu\")(layer)\n",
    "        layer = Dense(100, activation=\"relu\")(layer)\n",
    "        layer = Dense(20, activation=\"relu\")(layer) \n",
    "        \n",
    "        self.input_layers.append(layer1)\n",
    "        self.input_visibles.append(visible)\n",
    "    \n",
    "    def _output_layer(self, input_size):\n",
    "        row, col = input_size\n",
    "\n",
    "        layer = Dense(row, activation='relu')(self.inputs)\n",
    "        layer = Reshape((row, 1, 1))(layer)\n",
    "        layer = UpSampling2D((1, col))(layer)\n",
    "        layer = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(layer)\n",
    "\n",
    "        self.outputs_layers.append(layer)\n",
    "        \n",
    "    def _decoder(self):\n",
    "        x = Dense(100, activation=\"relu\")(self.inputs)\n",
    "        x = Dense(1000, activation=\"relu\")(x)\n",
    "        self.inputs = x\n",
    "        \n",
    "    def _callbacks(self):\n",
    "        log_dir = '/data/logs/'\n",
    "        self.callbacks = [\n",
    "            TensorBoard(log_dir=log_dir, histogram_freq=0, write_graph=True, write_images=True),\n",
    "            EarlyStopping(monitor='loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "        ]\n",
    "        \n",
    "    def build_model(self, optimizer='sgd', loss='mse'):\n",
    "        self.inputs = concatenate(self.input_layers)\n",
    "        self.inputs = Dense(20, activation=\"relu\")(self.inputs) \n",
    "        self._decoder()\n",
    "\n",
    "        [self._output_layer(data_size) for data_size in self.input_data_size]\n",
    "        \n",
    "        self.model = Model(inputs=self.input_visibles, outputs=self.outputs_layers)\n",
    "        self.model.compile(optimizer=optimizer, loss=loss)\n",
    "        self.model.summary()\n",
    "        \n",
    "    def fit(self, batch_size, epochs):\n",
    "        self.batch_size = batch_size;\n",
    "        self.model.fit_generator(\n",
    "            data_generator(self.filenames, batch_size), \n",
    "            steps_per_epoch=batch_size, \n",
    "            epochs=epochs,\n",
    "            callbacks=self.callbacks\n",
    "        )\n",
    "        \n",
    "    def save(self, path, version):\n",
    "        self.model.save(path + 'experiment_' + version + '.h5')\n",
    "\n",
    "    def load_model(self, path, version):\n",
    "        self.model = load_model(path + 'experiment_' + version + '.h5')\n",
    "        \n",
    "    def predict(self, random=False):\n",
    "        self.org_data = order_inputs(self.model, self.org_data)\n",
    "            \n",
    "        if random == True:\n",
    "            self.shuffled_data = replace_and_shuffle_data_with_random(self.org_data)\n",
    "        else: \n",
    "            self.shuffled_data = shuffle_data(self.org_data)\n",
    "            \n",
    "        self.predict_data = self.model.predict(self.org_data)\n",
    "        self.base_line = self.model.predict(self.shuffled_data)\n",
    "        outputHeader = 'Data\\t\\t'\n",
    "        outputHeader += 'Density\\t\\t'\n",
    "        outputHeader += 'Predict\\t\\t'\n",
    "        outputHeader += 'BaseLine\\t'\n",
    "        outputHeader += 'AVG Mean\\t\\t'\n",
    "        outputHeader += 'Predict (min)\\t'\n",
    "        outputHeader += 'Predict (max)\\t'\n",
    "        print(outputHeader)\n",
    "        \n",
    "#         print('Data \\t\\t\\tDensity \\tPredict \\tBaseLine \\tAVG Mean')\n",
    "        for i in range(len(self.org_data)):\n",
    "            _, row, col, _ = self.org_data[i].shape \n",
    "            mse = mean_squared_error(self.org_data[i].flatten(), self.predict_data[i].flatten())\n",
    "            mse_base_line = mean_squared_error(self.shuffled_data[i].flatten(), self.base_line[i].flatten())\n",
    "            \n",
    "            non_zeros = self.org_data[i].flatten()\n",
    "            non_zeros[self.org_data[i].flatten() > 0] = 1\n",
    "            org_mean = round(np.mean(non_zeros * self.org_data[i].flatten()), 5)\n",
    "            predict_mean = round(np.mean(non_zeros * self.predict_data[i].flatten()), 5)\n",
    "            \n",
    "            outputBody = '(' + str(row) + ',' + str(col) + ') ' + '\\t'\n",
    "            outputBody += str(round((np.count_nonzero(self.org_data[i])/(row * col)) * 100, 2)) + '% \\t\\t'\n",
    "            outputBody += str(round(mse * 100, 5)) + '% \\t'\n",
    "            outputBody += str(round(mse_base_line * 100, 5)) + '% '+ '\\t'\n",
    "            outputBody += str(org_mean) + ' - ' + str(predict_mean) + '\\t'\n",
    "            outputBody += str(round(min(self.predict_data[i].flatten()), 5)) + '\\t\\t'\n",
    "            outputBody += str(round(max(self.predict_data[i].flatten()), 5)) + '\\t'\n",
    "            print(outputBody)\n",
    "            \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiModal(graph=graph1, path='/data/samples/multiple_inputs/dicty/')\n",
    "model.build_model(optimizer='sgd', loss='mse')\n",
    "model.fit(500, 30)\n",
    "model.save('/data/multimodal/dicty/', str(numOfExperiment))\n",
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = MultiModal(graph=graph1, path='/data/samples/multiple_inputs/dicty/')\n",
    "model1.load_model('/data/multimodal/dicty/', str(numOfExperiment))\n",
    "model1.predict(random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = '/mag/Experiments/Results/'+ str(numOfExperiment) + '/dicty/'\n",
    "\n",
    "for i in range(len(model1.org_data)):\n",
    "    _, row, col, _ = model1.org_data[i].shape\n",
    "    org_data = model1.org_data[i].reshape(row, col)\n",
    "    shuffled_data = model1.shuffled_data[i].reshape(row, col)\n",
    "    predict = model1.predict_data[i].reshape(row, col)\n",
    "    base_line = model1.base_line[i].reshape(row, col)\n",
    "    print(org_data.shape)\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(16,16))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax1.title.set_text('OrgData')\n",
    "    plt.imshow(org_data)\n",
    "    ax1 = fig.add_subplot(1, 2, 2)\n",
    "    ax1.title.set_text('Predict')\n",
    "    plt.imshow(predict)\n",
    "    plt.colorbar()\n",
    "    plt.savefig(path + 'experiment' + str(numOfExperiment) + '_' + str(i) + '_org_vs_predict.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    fig = plt.figure(figsize=(16,16))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax1.title.set_text('ShuffledData')\n",
    "    plt.imshow(shuffled_data)\n",
    "    ax1 = fig.add_subplot(1, 2, 2)\n",
    "    ax1.title.set_text('BaseLine')\n",
    "    plt.imshow(base_line)\n",
    "    plt.colorbar()\n",
    "    plt.savefig(path + 'experiment' + str(numOfExperiment) + '_' + str(i) + '_randomData_vs_predictBaseLine.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    fig = plt.figure(figsize=(16,16))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax1.title.set_text('Predict - OrgData')\n",
    "    plt.imshow(np.abs(predict - org_data))\n",
    "    ax1 = fig.add_subplot(1, 2, 2)\n",
    "    ax1.title.set_text('BaseLine - ShuffledData')\n",
    "    plt.imshow(np.abs(base_line - shuffled_data))\n",
    "    plt.colorbar()\n",
    "    plt.savefig(path + 'experiment' + str(numOfExperiment) + '_' + str(i) + '_intersection.png')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiModal(graph=graph2, path='/data/samples/multiple_inputs/pharma/')\n",
    "model.build_model(optimizer='sgd', loss='mse')\n",
    "model.fit(500, 30)\n",
    "model.save('/data/multimodal/pharma/', str(numOfExperiment))\n",
    "model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MultiModal(graph=graph2, path='/data/samples/multiple_inputs/pharma/')\n",
    "model2.load_model('/data/multimodal/pharma/', str(numOfExperiment))\n",
    "model2.predict(random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = '/mag/Experiments/Results/'+ str(numOfExperiment) + '/pharma/'\n",
    "\n",
    "for i in range(len(model2.org_data)):\n",
    "    _, row, col, _ = model2.org_data[i].shape\n",
    "    org_data = model2.org_data[i].reshape(row, col)\n",
    "    shuffled_data = model2.shuffled_data[i].reshape(row, col)\n",
    "    predict = model2.predict_data[i].reshape(row, col)\n",
    "    base_line = model2.base_line[i].reshape(row, col)\n",
    "    print(org_data.shape)\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(16,16))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax1.title.set_text('OrgData')\n",
    "    plt.imshow(org_data)\n",
    "    ax1 = fig.add_subplot(1, 2, 2)\n",
    "    ax1.title.set_text('Predict')\n",
    "    plt.imshow(predict)\n",
    "    plt.colorbar()\n",
    "    plt.savefig(path + 'experiment' + str(numOfExperiment) + '_' + str(i) + '_org_vs_predict.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    fig = plt.figure(figsize=(16,16))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax1.title.set_text('ShuffledData')\n",
    "    plt.imshow(shuffled_data)\n",
    "    ax1 = fig.add_subplot(1, 2, 2)\n",
    "    ax1.title.set_text('BaseLine')\n",
    "    plt.imshow(base_line)\n",
    "    plt.colorbar()\n",
    "    plt.savefig(path + 'experiment' + str(numOfExperiment) + '_' + str(i) + '_randomData_vs_predictBaseLine.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    fig = plt.figure(figsize=(16,16))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax1.title.set_text('Predict - OrgData')\n",
    "    plt.imshow(np.abs(predict - org_data))\n",
    "    ax1 = fig.add_subplot(1, 2, 2)\n",
    "    ax1.title.set_text('BaseLine - ShuffledData')\n",
    "    plt.imshow(np.abs(base_line - shuffled_data))\n",
    "    plt.colorbar()\n",
    "    plt.savefig(path + 'experiment' + str(numOfExperiment) + '_' + str(i) + '_intersection.png')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
