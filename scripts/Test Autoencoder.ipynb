{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relationGraph import Relation, RelationGraph, MatrixOfRelationGraph\n",
    "from autoencoder import seedy, AutoEncoder\n",
    "import utilityFunctions as uf\n",
    "from main import test_build_relation_graph_with_symertic_data, test_convert_graph_to_2D_matrix, test_get_matix_for_autoencoder, test_autoencoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from base import load_source\n",
    "from os.path import join\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "0.0\n",
      "1.0\n",
      "-------------RelationGraph-------------\n",
      "Experimental condition\t282\n",
      "1\texpr_T-(282, 1219)\n",
      "1\texpr-(1219, 282)\n",
      "GO term\t116\n",
      "1\tann_T-(116, 1219)\n",
      "1\tann-(1219, 116)\n",
      "Gene\t1219\n",
      "3\tann-(1219, 116), expr-(1219, 282), ppi-(1219, 1219)\n",
      "2\tann_T-(116, 1219), expr_T-(282, 1219)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gene = 'Gene'\n",
    "go_term = 'GO term'\n",
    "exprc = 'Experimental condition'\n",
    "\n",
    "data, rn, cn = load_source(join('dicty', 'dicty.gene_annnotations.csv.gz'))\n",
    "data = uf.normalization(data)\n",
    "ann = Relation(data=data, x_name=gene, y_name=go_term, name='ann',\n",
    "               x_metadata=rn, y_metadata=cn)\n",
    "print(np.min(data))\n",
    "print(np.max(data))\n",
    "print()\n",
    "\n",
    "data, rn, cn = load_source(join('dicty', 'dicty.gene_expression.csv.gz'))\n",
    "expr = Relation(data=data, x_name=gene, y_name=exprc, name='expr',\n",
    "                x_metadata=rn, y_metadata=cn)\n",
    "expr.matrix = np.log(np.maximum(expr.matrix, np.finfo(np.float).eps))\n",
    "expr.matrix = uf.normalization(expr.matrix)\n",
    "print(np.min(expr.matrix))\n",
    "print(np.max(expr.matrix))\n",
    "print()\n",
    "\n",
    "data, rn, cn = load_source(join('dicty', 'dicty.ppi.csv.gz'))\n",
    "data = uf.normalization(data)\n",
    "ppi = Relation(data=data, x_name=gene, y_name=gene, name='ppi',\n",
    "               x_metadata=rn, y_metadata=cn)\n",
    "print(np.min(data))\n",
    "print(np.max(data))\n",
    "\n",
    "ann_t = ann.transpose()\n",
    "expr_t = expr.transpose()\n",
    "\n",
    "relationGraph = RelationGraph()\n",
    "relationGraph.add_relations([ann, expr, ppi, ann_t, expr_t])\n",
    "relationGraph.display_objects()\n",
    "graph = relationGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1219, 282)\n",
      "45959\n",
      "Min: -0.0999\n",
      "Max: 0.0\n",
      "45933\n",
      "-0.0999\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def normalization(data, _min=0, _max=1):\n",
    "    if _min >= _max:\n",
    "        raise ValueError('Attribute \\'min\\' must be lower than \\'min\\'.')\n",
    "    if _min > 0 or _max < 0:\n",
    "        raise ValueError('This operation is not supported!')\n",
    "    \n",
    "    min_val = np.min(data)    \n",
    "    if min_val < 0:\n",
    "        data = data + np.abs(min_val)\n",
    "        data[np.where(data == np.abs(min_val))] = 0\n",
    "        \n",
    "    max_val = np.max(data)\n",
    "    if max_val > 1:\n",
    "        data = data / max_val\n",
    "    elif max_val < 1:\n",
    "        factor = 1/max_val\n",
    "        data = data * factor\n",
    "        \n",
    "    return data\n",
    "\n",
    "print(expr.matrix.shape)\n",
    "print(np.count_nonzero(ppi.matrix))\n",
    "test = normalization(ppi.matrix)\n",
    "print(np.count_nonzero(test))\n",
    "print(np.min(ppi.matrix))\n",
    "print(np.max(ppi.matrix))\n",
    "print()\n",
    "print(np.min(test))\n",
    "print(np.max(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------ann_T (116, 1219)-----------\n",
      "(116, 1219)\n",
      "\n",
      "-----------ann (1219, 116)-----------\n",
      "x != y\n",
      "(1335, 1335)\n",
      "\n",
      "-----------expr (1219, 282)-----------\n",
      "x == 0\n",
      "(1335, 1617)\n",
      "\n",
      "-----------ppi (1219, 1219)-----------\n",
      "x == y\n",
      "(1335, 1617)\n",
      "\n",
      "-----------expr_T (282, 1219)-----------\n",
      "y == 0\n",
      "(1617, 1617)\n",
      "\n",
      "-------------2D Matrix-------------\n",
      "Objects: GO term: (0, (0, 115)), Gene: (1, (116, 1334)), Experimental condition: (2, (1335, 1616))\n",
      "[0. 1. 0.]\n",
      "[1. 1. 1.]\n",
      "[0. 1. 0.]\n",
      "\n",
      "GO term: 23\n",
      "Gene: 244\n",
      "Experimental condition: 56\n",
      "(323, 323)\n",
      "/data/samples/dicty/323_data.npz\n",
      "(323, 323)\n",
      "(323, 323)\n"
     ]
    }
   ],
   "source": [
    "mrg = MatrixOfRelationGraph(graph=graph)\n",
    "mrg.convert_to_2D_matrix()\n",
    "mrg.display_metadata_2D_matrix()\n",
    "data = mrg.density_data(.2)\n",
    "print(data.shape)\n",
    "fn = '/data/samples/dicty/' + str(data.shape[0]) + '_data.npz'\n",
    "# fn = '/data/samples/' + str(data.shape[0]) + '_ord_data.npz'    // original data for prediciton\n",
    "print(fn)\n",
    "print(data.shape)\n",
    "\n",
    "# f = np.load('/data/samples/dicty/162_org_data.npz')\n",
    "# data = f[f.files[0]]\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(323, 323)\n",
      "Finnish!!!\n"
     ]
    }
   ],
   "source": [
    "fn = '/data/samples/dicty/' + str(data.shape[0]) + '_data.npz'\n",
    "print(data.shape)\n",
    "# uf.data_generator(data, 50000, 14, 0.8, fn)\n",
    "# fn = '/data/samples/dicty/' + str(data.shape[0]) + '_org_data.npz'\n",
    "# f = uf.my_savez(fn)\n",
    "# f.savez(data)\n",
    "# f.close()\n",
    "\n",
    "print('Finnish!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162, 162)\n",
      "[[ 5.00661389  4.83041553  4.8573831  ... -0.0227      0.\n",
      "   0.        ]\n",
      " [ 4.86537828  4.71475975  4.96717789 ... -0.0245      0.\n",
      "   0.        ]\n",
      " [ 4.36110939  4.20911566  4.58180944 ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  1.50696212  4.75418484\n",
      "   3.51393065]\n",
      " [ 0.          0.          0.         ... -0.55686956  5.19988833\n",
      "   2.52003245]\n",
      " [ 0.          0.          0.         ...  0.79615493  4.66824827\n",
      "   4.0636104 ]]\n",
      "(162, 162)\n",
      "162\n",
      "/data/weights/12\n",
      "Epoch 1/200\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 4.7013\n",
      "Epoch 2/200\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 1.8918\n",
      "Epoch 3/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 1.3259\n",
      "Epoch 4/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 1.0178\n",
      "Epoch 5/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.8023\n",
      "Epoch 6/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6473\n",
      "Epoch 7/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5447\n",
      "Epoch 8/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4715\n",
      "Epoch 9/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4171\n",
      "Epoch 10/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.3603\n",
      "Epoch 11/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.3173\n",
      "Epoch 12/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.2832\n",
      "Epoch 13/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.2554\n",
      "Epoch 14/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.2320\n",
      "Epoch 15/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2124\n",
      "Epoch 16/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.1958\n",
      "Epoch 17/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.1814\n",
      "Epoch 18/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.1687\n",
      "Epoch 19/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.1676\n",
      "Epoch 20/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.1487\n",
      "Epoch 21/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.1407\n",
      "Epoch 22/200\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.1340\n",
      "Epoch 23/200\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.1281\n",
      "Epoch 24/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.1249\n",
      "Epoch 25/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.1220\n",
      "Epoch 26/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.1148\n",
      "Epoch 27/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.1116\n",
      "Epoch 28/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.1103\n",
      "Epoch 29/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.1065\n",
      "Epoch 30/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.1056\n",
      "Epoch 31/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.1040\n",
      "Epoch 32/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.1008\n",
      "Epoch 33/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.1026\n",
      "Epoch 34/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0991\n",
      "Epoch 35/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0991\n",
      "Epoch 36/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0970\n",
      "Epoch 37/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0957\n",
      "Epoch 38/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0946\n",
      "Epoch 39/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0939\n",
      "Epoch 40/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0949\n",
      "Epoch 41/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0920\n",
      "Epoch 42/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0911\n",
      "Epoch 43/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0905\n",
      "Epoch 44/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0912\n",
      "Epoch 45/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0908\n",
      "Epoch 46/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0891\n",
      "Epoch 47/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0889\n",
      "Epoch 48/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0882\n",
      "Epoch 49/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0882\n",
      "Epoch 50/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0885\n",
      "Epoch 51/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0867\n",
      "Epoch 52/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0864\n",
      "Epoch 53/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0863\n",
      "Epoch 54/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0857\n",
      "Epoch 55/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0853\n",
      "Epoch 56/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0873\n",
      "Epoch 57/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0852\n",
      "Epoch 58/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0846\n",
      "Epoch 59/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0842\n",
      "Epoch 60/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0838\n",
      "Epoch 61/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0841\n",
      "Epoch 62/200\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.0835\n",
      "Epoch 63/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0837\n",
      "Epoch 64/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0822\n",
      "Epoch 65/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0831\n",
      "Epoch 66/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0832\n",
      "Epoch 67/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0815\n",
      "Epoch 68/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0814\n",
      "Epoch 69/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0819\n",
      "Epoch 70/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0814\n",
      "Epoch 71/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0813\n",
      "Epoch 72/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0806\n",
      "Epoch 73/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0804\n",
      "Epoch 74/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0799\n",
      "Epoch 75/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0801\n",
      "Epoch 76/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0800\n",
      "Epoch 77/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0797\n",
      "Epoch 78/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0791\n",
      "Epoch 79/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0789\n",
      "Epoch 80/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0782\n",
      "Epoch 81/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0783A: 0s - l\n",
      "Epoch 82/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0782\n",
      "Epoch 83/200\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 0.0779\n",
      "Epoch 84/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0775\n",
      "Epoch 85/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0769\n",
      "Epoch 86/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0774\n",
      "Epoch 87/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0769\n",
      "Epoch 88/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0765\n",
      "Epoch 89/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0756\n",
      "Epoch 90/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0759\n",
      "Epoch 91/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0756\n",
      "Epoch 92/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0751\n",
      "Epoch 93/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0750\n",
      "Epoch 94/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0747\n",
      "Epoch 95/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0743\n",
      "Epoch 96/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0741\n",
      "Epoch 97/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0738\n",
      "Epoch 98/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0734\n",
      "Epoch 99/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0732\n",
      "Epoch 100/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0729\n",
      "Epoch 101/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0725\n",
      "Epoch 102/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0722\n",
      "Epoch 103/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0719\n",
      "Epoch 104/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0716\n",
      "Epoch 105/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0713\n",
      "Epoch 106/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0710\n",
      "Epoch 107/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0706\n",
      "Epoch 108/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0703\n",
      "Epoch 109/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0700\n",
      "Epoch 110/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0697\n",
      "Epoch 111/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0694\n",
      "Epoch 112/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0691\n",
      "Epoch 113/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0687\n",
      "Epoch 114/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0684\n",
      "Epoch 115/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0681\n",
      "Epoch 116/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0678\n",
      "Epoch 117/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0675\n",
      "Epoch 118/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0671\n",
      "Epoch 119/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0668\n",
      "Epoch 120/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0665\n",
      "Epoch 121/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0662\n",
      "Epoch 122/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0659\n",
      "Epoch 123/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0656\n",
      "Epoch 124/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0653\n",
      "Epoch 125/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0650\n",
      "Epoch 126/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0647\n",
      "Epoch 127/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0644\n",
      "Epoch 128/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0641\n",
      "Epoch 129/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0638\n",
      "Epoch 130/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0635\n",
      "Epoch 131/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0632\n",
      "Epoch 132/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0630\n",
      "Epoch 133/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0627\n",
      "Epoch 134/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0624\n",
      "Epoch 135/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0621\n",
      "Epoch 136/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0619\n",
      "Epoch 137/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0616\n",
      "Epoch 138/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0613\n",
      "Epoch 139/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0611\n",
      "Epoch 140/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0608\n",
      "Epoch 141/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0605\n",
      "Epoch 142/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0603\n",
      "Epoch 143/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0600\n",
      "Epoch 144/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0598\n",
      "Epoch 145/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0595\n",
      "Epoch 146/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0594\n",
      "Epoch 147/200\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 0.0592\n",
      "Epoch 148/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0586\n",
      "Epoch 149/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0597\n",
      "Epoch 150/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0578\n",
      "Epoch 151/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0591\n",
      "Epoch 152/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0579\n",
      "Epoch 153/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0573\n",
      "Epoch 154/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0581\n",
      "Epoch 155/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0573\n",
      "Epoch 156/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0568\n",
      "Epoch 157/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0573\n",
      "Epoch 158/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0568\n",
      "Epoch 159/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0563\n",
      "Epoch 160/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0566\n",
      "Epoch 161/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0564\n",
      "Epoch 162/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0559\n",
      "Epoch 163/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0559\n",
      "Epoch 164/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0560\n",
      "Epoch 165/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0555\n",
      "Epoch 166/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0554\n",
      "Epoch 167/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0555\n",
      "Epoch 168/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0553\n",
      "Epoch 169/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0550\n",
      "Epoch 170/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0550\n",
      "Epoch 171/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0550\n",
      "Epoch 172/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0549\n",
      "Epoch 173/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0545\n",
      "Epoch 174/200\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.0542\n",
      "Epoch 175/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0552\n",
      "Epoch 176/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0551\n",
      "Epoch 177/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0538\n",
      "Epoch 178/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0536\n",
      "Epoch 179/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0543\n",
      "Epoch 180/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0554\n",
      "Epoch 181/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0536\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "(162, 162)\n",
      "MSE: 0.05350979247619257\n"
     ]
    }
   ],
   "source": [
    "fn = '/data/samples/162_org_data.npz'\n",
    "f = np.load(fn)\n",
    "data = f[f.files[0]]\n",
    "print(data.shape)\n",
    "\n",
    "# fn = '/data/samples/162_data.npz'\n",
    "\n",
    "x, y = data.shape\n",
    "# data=data.reshape(1, x * y)\n",
    "ae = AutoEncoder(encoding_dim=20, data=data)\n",
    "ae.encoder_decoder()\n",
    "# ae.fit(batch_size=250, epochs=100)\n",
    "# fn = '/mag/483_data.npz'\n",
    "ae.fit_generator(fn, n_packs=300, epochs=200)\n",
    "ae.save()\n",
    "\n",
    "encoder = ae.load_encoder()\n",
    "decoder = ae.load_decoder()\n",
    "\n",
    "# test_data = np.asarray([data[0].flatten()])\n",
    "# test_data = np.asarray([data.flatten()])\n",
    "test_data = data\n",
    "# print(test_data)\n",
    "# np.random.shuffle(test_data[0])\n",
    "# print(test_data)\n",
    "print(test_data.shape)\n",
    "\n",
    "x = encoder.predict(test_data)\n",
    "y = decoder.predict(x)\n",
    "\n",
    "mse = mean_squared_error(test_data, y)\n",
    "print('MSE: ' + str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_2 (Masking)          (None, 2604996)           0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 30)                78149910  \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2604996)           80754876  \n",
      "=================================================================\n",
      "Total params: 158,904,786\n",
      "Trainable params: 158,904,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "50/50 [==============================] - 330s 7s/step - loss: 16.8331\n",
      "Epoch 2/200\n",
      "50/50 [==============================] - 325s 7s/step - loss: 6.0696\n",
      "Epoch 3/200\n",
      "50/50 [==============================] - 331s 7s/step - loss: 3.4019\n",
      "Epoch 4/200\n",
      "50/50 [==============================] - 325s 6s/step - loss: 3.4014\n",
      "Epoch 5/200\n",
      "50/50 [==============================] - 330s 7s/step - loss: 3.4015\n",
      "Epoch 6/200\n",
      "50/50 [==============================] - 331s 7s/step - loss: 3.4015\n",
      "Epoch 7/200\n",
      "50/50 [==============================] - 326s 7s/step - loss: 3.4013\n",
      "Epoch 8/200\n",
      "50/50 [==============================] - 320s 6s/step - loss: 3.4012\n",
      "Epoch 9/200\n",
      "50/50 [==============================] - 319s 6s/step - loss: 3.4020\n",
      "Epoch 10/200\n",
      "50/50 [==============================] - 312s 6s/step - loss: 3.4016\n",
      "Epoch 11/200\n",
      "36/50 [====================>.........] - ETA: 1:31 - loss: 3.4019"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Masking\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "import autoencoder as ae\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "# num = 81   # most densely filled  0.05\n",
    "# num = 162   # most densely filled  0.1\n",
    "# num = 323   # most densely filled  0.2\n",
    "# num = 483   # most densely filled  0.3\n",
    "# num = 645   # most densely filled  0.4\n",
    "# num = 807   # most densely filled  0.5\n",
    "num = 1614  # all data\n",
    "\n",
    "fn = '/data/samples/dicty/' + str(num) + '_org_data.npz'\n",
    "f = np.load(fn)\n",
    "data = f[f.files[0]]\n",
    "\n",
    "fn = '/data/samples/dicty/' + str(num) + '_data.npz'\n",
    "        \n",
    "x,y = data.shape\n",
    "data=data.reshape(1, x * y)\n",
    "input_dim = data.shape[1]\n",
    "epochs = 200\n",
    "encoding_dim = 30\n",
    "n_packs = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0, input_shape=(input_dim, )))\n",
    "# model.add(Dense(int(input_dim / 2), activation='relu'))\n",
    "model.add(Dense(encoding_dim, input_shape=(input_dim, ), activation='relu'))\n",
    "# model.add(Dense(int(input_dim / 2), activation='relu'))\n",
    "model.add(Dense(input_dim))\n",
    "model.compile(loss='mse', optimizer='sgd')\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Masking(mask_value=0, input_shape=(input_dim, )))\n",
    "# model.add(Dense(int(input_dim / 2), activation='relu'))\n",
    "# model.add(Dense(int(input_dim / 4), activation='relu'))\n",
    "# model.add(Dense(encoding_dim, activation='relu'))\n",
    "# model.add(Dense(int(input_dim / 4), activation='relu'))\n",
    "# model.add(Dense(int(input_dim / 2), activation='relu'))\n",
    "# model.add(Dense(input_dim))\n",
    "# model.compile(loss='mse', optimizer='sgd')\n",
    "model.summary()\n",
    "\n",
    "\n",
    "log_dir = '/data/logs/'\n",
    "callbacks = [\n",
    "            TensorBoard(log_dir=log_dir, histogram_freq=0, write_graph=True, write_images=True),\n",
    "            EarlyStopping(monitor='loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "        ]\n",
    "\n",
    "model.fit_generator(ae.data_generator(fn, n_packs), steps_per_epoch=n_packs, epochs=epochs, callbacks=callbacks)\n",
    "\n",
    "model.save('/data/sequential/weights/' + str(num) + '_model.h5')\n",
    "\n",
    "decoded_imgs = model.predict(data)\n",
    "\n",
    "mse = mean_squared_error(data, decoded_imgs)\n",
    "print('MSE: ' + str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.31138956 4.33326959 4.7106287  ... 4.80063006 2.11492933 4.1626105 ]\n",
      "[4.2677693 4.4455185 4.820116  ... 4.86551   2.086997  4.0896974]\n",
      "\n",
      "(1, 104329)\n",
      "(1, 104329)\n",
      "\n",
      "MSE org data: 0.012063925241763864\n",
      "MSE shuffled data: 45.73822851526672\n",
      "\n",
      "\n",
      "Min org data:-36.04365338911715\n",
      "Max org data:10.068331257647785\n",
      "Mean org data: 1.4092668375654196\n",
      "\n",
      "Min predict:-2.080417\n",
      "Max predict:0.77436584\n",
      "Mean predict: 0.060567502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# num = 162   # most densely filled  0.1\n",
    "num = 323   # most densely filled  0.2\n",
    "# num = 483   # most densely filled  0.3\n",
    "# num = 645   # most densely filled  0.4\n",
    "# num = 807   # most densely filled  0.5\n",
    "\n",
    "f = np.load('/data/samples/' + str(num) + '_org_data.npz')\n",
    "test_data = np.asarray([f[f.files[0]].flatten()])\n",
    "# test_data = np.asarray([data.flatten()])\n",
    "\n",
    "# prediction with normal data\n",
    "model = load_model('/data/sequential/weights/' + str(num) + '_model.h5')\n",
    "y = model.predict(test_data)\n",
    "mse = mean_squared_error(test_data, y)\n",
    "print(test_data[0])\n",
    "print(y[0])\n",
    "print()\n",
    "print(test_data.shape)\n",
    "print(y.shape)\n",
    "print()\n",
    "print('MSE org data: ' + str(mse))\n",
    "\n",
    "\n",
    "# prediction with shuffled data\n",
    "np.random.shuffle(test_data[0])\n",
    "y = model.predict(test_data)\n",
    "mse = mean_squared_error(test_data, y)\n",
    "print('MSE shuffled data: ' + str(mse))\n",
    "print()\n",
    "# print('Mean predict data: ' + str(np.mean(y[0])))\n",
    "# print(test_data[0])\n",
    "# print(y[0])\n",
    "print()\n",
    "print('Min org data:' + str(np.min(test_data)))\n",
    "print('Max org data:' + str(np.max(test_data)))\n",
    "print('Mean org data: ' + str(np.mean(test_data)))\n",
    "print()\n",
    "print('Min predict:' + str(np.min(y)))\n",
    "print('Max predict:' + str(np.max(y)))\n",
    "print('Mean predict: ' + str(np.mean(y)))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
