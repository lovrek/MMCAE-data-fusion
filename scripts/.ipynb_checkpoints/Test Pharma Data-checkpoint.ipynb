{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relationGraph import Relation, RelationGraph, MatrixOfRelationGraph\n",
    "from autoencoder import seedy, AutoEncoder\n",
    "import utilityFunctions as uf\n",
    "from main import test_build_relation_graph_with_symertic_data, test_convert_graph_to_2D_matrix, test_get_matix_for_autoencoder, test_autoencoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from base import load_source\n",
    "from os.path import join\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "0.0\n",
      "1.0\n",
      "\n",
      "-------------RelationGraph-------------\n",
      "Action\t130\n",
      "1\tactions_T-(130, 1260)\n",
      "1\tactions-(1260, 130)\n",
      "PMID\t7948\n",
      "1\tpudmed_T-(7948, 1260)\n",
      "1\tpudmed-(1260, 7948)\n",
      "Depositor category\t16\n",
      "1\tdepo_cats_T-(16, 189)\n",
      "1\tdepo_cats-(189, 16)\n",
      "Depositor\t189\n",
      "2\tdepo_cats-(189, 16), depositors_T-(189, 1260)\n",
      "2\tdepositors-(1260, 189), depo_cats_T-(16, 189)\n",
      "Chemical\t1260\n",
      "5\tactions-(1260, 130), pudmed-(1260, 7948), depositors-(1260, 189), fingerprints-(1260, 920), tanimoto-(1260, 1260)\n",
      "4\tactions_T-(130, 1260), pudmed_T-(7948, 1260), depositors_T-(189, 1260), fingerprints_T-(920, 1260)\n",
      "Fingerprint\t920\n",
      "1\tfingerprints_T-(920, 1260)\n",
      "1\tfingerprints-(1260, 920)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def replace_duplicate(duplicates, arr, term='--'):\n",
    "    arr = np.array(arr, np.dtype('<U10'))\n",
    "#     print(duplicates)\n",
    "    for element in duplicates:\n",
    "        idx = np.where(arr == element)\n",
    "        arr[idx[0][0]] = str(element) + term\n",
    "#         print(element)\n",
    "#         print(idx)\n",
    "#         print(arr[idx[0][0]])\n",
    "#         print()\n",
    "    return arr\n",
    "\n",
    "action='Action'\n",
    "pmid='PMID'\n",
    "depositor='Depositor'\n",
    "fingerprint='Fingerprint'\n",
    "depo_cat='Depositor category'\n",
    "chemical='Chemical'\n",
    "\n",
    "data, rn, cn = load_source(join('pharma', 'pharma.actions.csv.gz'))\n",
    "actions = Relation(data=data, x_name=chemical, y_name=action, name='actions',\n",
    "                   x_metadata=rn, y_metadata=cn)\n",
    "print(np.min(data))\n",
    "print(np.max(data))\n",
    "print()\n",
    "a = cn\n",
    "data, rn, cn = load_source(join('pharma', 'pharma.pubmed.csv.gz'))\n",
    "cn = replace_duplicate(set(rn) & set(cn), cn, '--')\n",
    "pubmed = Relation(data=data, x_name=chemical, y_name=pmid, name='pudmed',\n",
    "                  x_metadata=rn, y_metadata=cn)\n",
    "p = cn\n",
    "print(np.min(data))\n",
    "print(np.max(data))\n",
    "print()\n",
    "\n",
    "data, rn, cn = load_source(join('pharma', 'pharma.depositors.csv.gz'))\n",
    "depositors = Relation(data=data, x_name=chemical, y_name=depositor, name='depositors',\n",
    "                      x_metadata=rn, y_metadata=cn)\n",
    "d = cn\n",
    "print(np.min(data))\n",
    "print(np.max(data))\n",
    "print()\n",
    "data, rn, cn = load_source(join('pharma', 'pharma.fingerprints.csv.gz'))\n",
    "cn = replace_duplicate(set(rn) & set(cn), cn, '**')\n",
    "cn = replace_duplicate(set(p) & set(cn), cn, '**')\n",
    "fingerprints = Relation(data=data, x_name=chemical, y_name=fingerprint, name='fingerprints',\n",
    "                        x_metadata=rn, y_metadata=cn)\n",
    "f = cn\n",
    "print(np.min(data))\n",
    "print(np.max(data))\n",
    "print()\n",
    "data, rn, cn = load_source(join('pharma', 'pharma.depo_cats.csv.gz'))\n",
    "depo_cats = Relation(data=data, x_name=depositor, y_name=depo_cat, name='depo_cats',\n",
    "                     x_metadata=rn, y_metadata=cn)\n",
    "dc = cn\n",
    "print(np.min(data))\n",
    "print(np.max(data))\n",
    "print()\n",
    "data, rn, cn = load_source(join('pharma', 'pharma.tanimoto.csv.gz'))\n",
    "data = uf.normalization(data)\n",
    "tanimoto = Relation(data=data, x_name=chemical, y_name=chemical, name='tanimoto',\n",
    "                    x_metadata=rn, y_metadata=cn)\n",
    "c = cn\n",
    "print(np.min(data))\n",
    "print(np.max(data))\n",
    "print()\n",
    "actions_t = actions.transpose()\n",
    "pubmed_t = pubmed.transpose()\n",
    "depositors_t = depositors.transpose()\n",
    "fingerprints_t = fingerprints.transpose()\n",
    "depo_cats_t = depo_cats.transpose()\n",
    "\n",
    "relationGraph = RelationGraph()\n",
    "relationGraph.add_relations([\n",
    "    actions,\n",
    "    pubmed,\n",
    "    depositors,\n",
    "    fingerprints,\n",
    "    depo_cats,\n",
    "    tanimoto,\n",
    "    actions_t,\n",
    "    pubmed_t,\n",
    "    depositors_t,\n",
    "    fingerprints_t,\n",
    "    depo_cats_t\n",
    "])\n",
    "relationGraph.display_objects()\n",
    "graph = relationGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------actions_T (130, 1260)-----------\n",
      "(130, 1260)\n",
      "\n",
      "-----------actions (1260, 130)-----------\n",
      "x != y\n",
      "(1390, 1390)\n",
      "\n",
      "-----------pudmed_T (7948, 1260)-----------\n",
      "y == 0\n",
      "(9338, 1390)\n",
      "\n",
      "-----------pudmed (1260, 7948)-----------\n",
      "x == 0\n",
      "(9338, 9338)\n",
      "\n",
      "-----------depo_cats_T (16, 189)-----------\n",
      "x != y\n",
      "(9354, 9527)\n",
      "\n",
      "-----------depo_cats (189, 16)-----------\n",
      "x != y\n",
      "(9543, 9543)\n",
      "\n",
      "-----------depositors_T (189, 1260)-----------\n",
      "x == y\n",
      "(9543, 9543)\n",
      "\n",
      "-----------depositors (1260, 189)-----------\n",
      "x == y\n",
      "(9543, 9543)\n",
      "\n",
      "-----------fingerprints (1260, 920)-----------\n",
      "x == 0\n",
      "(9543, 10463)\n",
      "\n",
      "-----------tanimoto (1260, 1260)-----------\n",
      "x == y\n",
      "(9543, 10463)\n",
      "\n",
      "-----------fingerprints_T (920, 1260)-----------\n",
      "y == 0\n",
      "(10463, 10463)\n",
      "\n",
      "-------------2D Matrix-------------\n",
      "Objects: Action: (0, (0, 129)), PMID: (1, (130, 8077)), Depositor: (3, (8094, 8282)), Chemical: (4, (8283, 9542)), Fingerprint: (5, (9543, 10462)), Depositor category: (2, (8078, 8093))\n",
      "[0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 0. 1. 0.]\n",
      "[0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 1. 0. 1. 0.]\n",
      "[1. 1. 0. 1. 1. 1.]\n",
      "[0. 0. 0. 0. 1. 0.]\n",
      "\n",
      "Action: 13\n",
      "PMID: 795\n",
      "Depositor: 19\n",
      "Chemical: 126\n",
      "Fingerprint: 92\n",
      "Depositor category: 2\n"
     ]
    }
   ],
   "source": [
    "# print(set(a) & set(c))\n",
    "# print(set(a) & set(d))\n",
    "# print(set(a) & set(p))\n",
    "# print(set(a) & set(f))\n",
    "# print(set(a) & set(dc))\n",
    "# print()\n",
    "# print(set(c) & set(d))\n",
    "# print(set(c) & set(p))\n",
    "# print(set(c) & set(f))\n",
    "# print(set(c) & set(dc))\n",
    "# print()\n",
    "# print(set(d) & set(p))\n",
    "# print(set(d) & set(f))\n",
    "# print(set(d) & set(dc))\n",
    "# print()\n",
    "# print(set(p) & set(f))\n",
    "# print(set(p) & set(dc))\n",
    "# print()\n",
    "# print(set(f) & set(dc))\n",
    "\n",
    "\n",
    "\n",
    "mrg = MatrixOfRelationGraph(graph=graph)\n",
    "mrg.convert_to_2D_matrix()\n",
    "mrg.display_metadata_2D_matrix()\n",
    "data = mrg.density_data(.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed: 684\n",
      "Random seed: 235\n",
      "Random seed: 527\n",
      "Random seed: 952\n",
      "Random seed: 174\n",
      "Random seed: 867\n",
      "Random seed: 906\n",
      "Random seed: 175\n",
      "Random seed: 451\n",
      "Random seed: 382\n",
      "Random seed: 527\n",
      "Random seed: 332\n",
      "Random seed: 432\n",
      "Random seed: 338\n",
      "Random seed: 619\n",
      "Random seed: 645\n",
      "Random seed: 681\n",
      "Random seed: 623\n",
      "Random seed: 275\n",
      "Random seed: 622\n",
      "Random seed: 474\n",
      "Random seed: 969\n",
      "Random seed: 885\n",
      "Random seed: 595\n",
      "Random seed: 418\n",
      "Random seed: 318\n",
      "samples: 100\n",
      "Random seed: 821\n",
      "Random seed: 1043\n"
     ]
    }
   ],
   "source": [
    "_dir = 'pharma'\n",
    "\n",
    "fn = '/data/samples/' + _dir + '/' + str(data.shape[0]) + '_data.npz'\n",
    "uf.data_generator(data, 500000, 14, 0.8, fn)\n",
    "\n",
    "# fn = '/data/samples/' + _dir + '/' + str(data.shape[0]) + '_org_data.npz'\n",
    "# f = uf.my_savez(fn)\n",
    "# f.savez(data)\n",
    "# f.close()\n",
    "\n",
    "print('Finnish!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_1 (Masking)          (None, 1096209)           0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                21924200  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1096209)           23020389  \n",
      "=================================================================\n",
      "Total params: 44,944,589\n",
      "Trainable params: 44,944,589\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "50/50 [==============================] - 169s 3s/step - loss: 0.3291\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 165s 3s/step - loss: 0.3290\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 168s 3s/step - loss: 0.3290\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 166s 3s/step - loss: 0.3290\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 165s 3s/step - loss: 0.3290\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 165s 3s/step - loss: 0.3290\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 165s 3s/step - loss: 0.3290\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 167s 3s/step - loss: 0.3290\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 164s 3s/step - loss: 0.3290\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 171s 3s/step - loss: 0.3290\n",
      "MSE: 0.41109924299983513\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Masking\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "import autoencoder as ae\n",
    "\n",
    "num = 1047   # most densely filled  0.1\n",
    "_dir = 'pharma'\n",
    "\n",
    "fn = '/data/samples/' + _dir + '/' + str(num) + '_org_data.npz'\n",
    "f = np.load(fn)\n",
    "data = f[f.files[0]]\n",
    "\n",
    "fn = '/data/samples/'+ _dir + '/' + str(num) + '_data.npz'\n",
    "        \n",
    "x,y = data.shape\n",
    "data=data.reshape(1, x * y)\n",
    "input_dim = data.shape[1]\n",
    "epochs = 10\n",
    "encoding_dim = 20\n",
    "n_packs = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0, input_shape=(input_dim, )))\n",
    "# model.add(Dense(int(input_dim / 2), activation='relu'))\n",
    "model.add(Dense(encoding_dim, input_shape=(input_dim, ), activation='relu'))\n",
    "# model.add(Dense(int(input_dim / 2), activation='relu'))\n",
    "model.add(Dense(input_dim))\n",
    "model.compile(loss='mse', optimizer='sgd')\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Masking(mask_value=0, input_shape=(input_dim, )))\n",
    "# model.add(Dense(int(input_dim / 2), activation='relu'))\n",
    "# model.add(Dense(int(input_dim / 4), activation='relu'))\n",
    "# model.add(Dense(encoding_dim, activation='relu'))\n",
    "# model.add(Dense(int(input_dim / 4), activation='relu'))\n",
    "# model.add(Dense(int(input_dim / 2), activation='relu'))\n",
    "# model.add(Dense(input_dim))\n",
    "# model.compile(loss='mse', optimizer='sgd')\n",
    "model.summary()\n",
    "\n",
    "\n",
    "log_dir = '/data/logs/'\n",
    "callbacks = [\n",
    "            TensorBoard(log_dir=log_dir, histogram_freq=0, write_graph=True, write_images=True),\n",
    "            EarlyStopping(monitor='loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "        ]\n",
    "\n",
    "model.fit_generator(\n",
    "    ae.data_generator(fn, n_packs), \n",
    "    steps_per_epoch=n_packs, \n",
    "    epochs=epochs, \n",
    "    callbacks=callbacks)\n",
    "\n",
    "model.save('/data/sequential/weights/' + str(num) + '_model.h5')\n",
    "\n",
    "decoded_imgs = model.predict(data)\n",
    "\n",
    "mse = mean_squared_error(data, decoded_imgs)\n",
    "print('MSE: ' + str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 6561)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 250)          1640500     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 20)           5020        dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 20)           5020        dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "kl_divergence_layer_3 (KLDiverg [(None, 20), (None,  0           dense_18[0][0]                   \n",
      "                                                                 dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 20)           0           kl_divergence_layer_3[0][1]      \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 20)           0           lambda_3[0][0]                   \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 20)           0           kl_divergence_layer_3[0][0]      \n",
      "                                                                 multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 6561)         1652061     add_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 3,302,601\n",
      "Trainable params: 3,302,601\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 650656997039.0521 - mean_squared_error: 5.3246\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 1s 10ms/step - loss: -86566.8230 - mean_squared_error: 5.3087\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 1s 10ms/step - loss: -87178.0566 - mean_squared_error: 5.3097\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 1s 9ms/step - loss: -86637.4207 - mean_squared_error: 5.3091\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 12967045727.4604 - mean_squared_error: 5.3144\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 1s 9ms/step - loss: -86509.9727 - mean_squared_error: 5.3167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f313d7b34a8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import autoencoder as ae\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Layer, Add, Multiply, Masking\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# from tensorflow.keras.models import Sequential, load_model\n",
    "# from tensorflow.keras.layers import Dense, Masking\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "\n",
    "\n",
    "num = 81    # most densely filled  0.05\n",
    "# num = 162   # most densely filled  0.1\n",
    "# num = 323   # most densely filled  0.2\n",
    "# num = 483   # most densely filled  0.3\n",
    "# num = 645   # most densely filled  0.4\n",
    "# num = 807   # most densely filled  0.5\n",
    "\n",
    "\n",
    "# original_dim = 784\n",
    "# intermediate_dim = 256\n",
    "original_dim = np.power(num, 2)\n",
    "# intermediate_dim = int(original_dim / 3.)\n",
    "intermediate_dim = 250\n",
    "latent_dim = 20\n",
    "batch_size = 100\n",
    "epochs = 50\n",
    "epsilon_std = .5\n",
    "n_packs = batch_size\n",
    "\n",
    "f = np.load('/data/samples/dicty/' + str(num) + '_org_data.npz')\n",
    "data = f[f.files[0]]\n",
    "fn = '/data/samples/dicty/' + str(num) + '_data.npz'\n",
    "\n",
    "log_dir = '/data/logs/'\n",
    "\n",
    "\n",
    "def nll(y_true, y_pred):\n",
    "    \"\"\" Negative log likelihood (Bernoulli). \"\"\"\n",
    "\n",
    "    # keras.losses.binary_crossentropy gives the mean\n",
    "    # over the last axis. we require the sum\n",
    "    return K.sum(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "\n",
    "\n",
    "class KLDivergenceLayer(Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        mu, log_var = inputs\n",
    "\n",
    "        kl_batch = - .5 * K.sum(1 + log_var -\n",
    "                                K.square(mu) -\n",
    "                                K.exp(log_var), axis=-1)\n",
    "\n",
    "        self.add_loss(K.mean(kl_batch), inputs=inputs)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "\n",
    "decoder = Sequential([\n",
    "    Dense(intermediate_dim, input_dim=latent_dim, activation='relu'),\n",
    "    Dense(original_dim, activation='sigmoid')\n",
    "])\n",
    "\n",
    "x = Input(shape=(original_dim,))\n",
    "h = Dense(intermediate_dim, activation='relu')(x)\n",
    "\n",
    "z_mu = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "z_mu, z_log_var = KLDivergenceLayer()([z_mu, z_log_var])\n",
    "z_sigma = Lambda(lambda t: K.exp(.5*t))(z_log_var)\n",
    "\n",
    "eps = Input(tensor=K.random_normal(stddev=epsilon_std,\n",
    "                                   shape=(K.shape(x)[0], latent_dim)))\n",
    "z_eps = Multiply()([z_sigma, eps])\n",
    "z = Add()([z_mu, z_eps])\n",
    "\n",
    "x_pred = decoder(z)\n",
    "\n",
    "vae = Model(inputs=[x, eps], outputs=x_pred)\n",
    "vae.compile(optimizer='rmsprop', loss=nll, metrics=[\"mean_squared_error\"])\n",
    "vae.summary()\n",
    "\n",
    "# train the VAE on MNIST digits\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# x_train = x_train.reshape(-1, original_dim) / 255.\n",
    "# x_test = x_test.reshape(-1, original_dim) / 255.\n",
    "\n",
    "# vae.fit(x_train,\n",
    "#         x_train,\n",
    "#         shuffle=True,\n",
    "#         epochs=epochs,\n",
    "#         batch_size=batch_size,\n",
    "#         validation_data=(x_test, x_test))\n",
    "\n",
    "callbacks = [\n",
    "            TensorBoard(log_dir=log_dir, histogram_freq=0, write_graph=True, write_images=True),\n",
    "            EarlyStopping(monitor='loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "        ]\n",
    "\n",
    "vae.fit_generator(\n",
    "    ae.data_generator(fn, n_packs), \n",
    "    steps_per_epoch=n_packs,\n",
    "    callbacks=callbacks,\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "# encoder = Model(x, z_mu)\n",
    "\n",
    "# f = np.load(filename)\n",
    "# files = f.files\n",
    "# numOfFiles = 10\n",
    "# x_test = np.empty((0, np.power(num, 2)))\n",
    "# while numOfFiles > 0:\n",
    "#     x_test = np.r_[x_test, f[files[numOfFiles]]]\n",
    "#     numOfFiles -= 1\n",
    "    \n",
    "# predict_data = vae.predict(data.flatten())\n",
    "# print(predict_data.shape)\n",
    "# mse = mean_squared_error(data, predict_data)\n",
    "# print('MSE: ' + str(mse))\n",
    "\n",
    "# display a 2D plot of the digit classes in the latent space\n",
    "# z_test = encoder.predict(x_test, batch_size=batch_size)\n",
    "# plt.figure(figsize=(6, 6))\n",
    "# plt.scatter(z_test[:, 0], z_test[:, 1], c=y_test,\n",
    "#             alpha=.4, s=3**2, cmap='viridis')\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "# display a 2D manifold of the digits\n",
    "# n = 15  # figure with 15x15 digits\n",
    "# digit_size = 28\n",
    "\n",
    "# linearly spaced coordinates on the unit square were transformed\n",
    "# through the inverse CDF (ppf) of the Gaussian to produce values\n",
    "# of the latent variables z, since the prior of the latent space\n",
    "# is Gaussian\n",
    "# u_grid = np.dstack(np.meshgrid(np.linspace(0.05, 0.95, n),\n",
    "#                                np.linspace(0.05, 0.95, n)))\n",
    "# z_grid = norm.ppf(u_grid)\n",
    "# x_decoded = decoder.predict(z_grid.reshape(n*n, 2))\n",
    "# x_decoded = x_decoded.reshape(n, n, digit_size, digit_size)\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.imshow(np.block(list(map(list, x_decoded))), cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (100, 26244)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (100, 250)           6561250     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (100, 250)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (100, 250)           62750       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (100, 250)           400         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (100, 250)           62750       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (100, 20)            5020        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (100, 20)            5020        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (100, 20)            0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (100, 250)           5250        lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (100, 250)           62750       dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (100, 250)           0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (100, 250)           62750       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (100, 26244)         6587244     dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 13,415,184\n",
      "Trainable params: 13,414,984\n",
      "Non-trainable params: 200\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "20/20 [==============================] - 4s 223ms/step - loss: 4483513190.4000 - categorical_accuracy: 0.0000e+00 - top_k_categorical_accuracy: 0.4000 - mean_squared_error: 38.1551\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 2958949504.0000 - categorical_accuracy: 0.0000e+00 - top_k_categorical_accuracy: 1.0000 - mean_squared_error: 37.4955\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 1s 68ms/step - loss: 2952423577.6000 - categorical_accuracy: 0.0000e+00 - top_k_categorical_accuracy: 1.0000 - mean_squared_error: 37.4931\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 1s 64ms/step - loss: 2942734528.0000 - categorical_accuracy: 0.0000e+00 - top_k_categorical_accuracy: 1.0000 - mean_squared_error: 37.5075\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 1s 60ms/step - loss: 2942296780.8000 - categorical_accuracy: 0.0000e+00 - top_k_categorical_accuracy: 1.0000 - mean_squared_error: 37.4962\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 1s 54ms/step - loss: 2941011264.0000 - categorical_accuracy: 0.0000e+00 - top_k_categorical_accuracy: 1.0000 - mean_squared_error: 37.4956\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 1s 70ms/step - loss: 2938653862.4000 - categorical_accuracy: 0.0000e+00 - top_k_categorical_accuracy: 0.9995 - mean_squared_error: 37.4994\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 1s 61ms/step - loss: 2937674342.4000 - categorical_accuracy: 0.0000e+00 - top_k_categorical_accuracy: 1.0000 - mean_squared_error: 37.5047\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 2936232588.8000 - categorical_accuracy: 0.0000e+00 - top_k_categorical_accuracy: 1.0000 - mean_squared_error: 37.5125\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 1s 58ms/step - loss: 2935634752.0000 - categorical_accuracy: 0.0000e+00 - top_k_categorical_accuracy: 0.9990 - mean_squared_error: 37.5090\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 1s 57ms/step - loss: 2939102374.4000 - categorical_accuracy: 0.0000e+00 - top_k_categorical_accuracy: 0.9960 - mean_squared_error: 37.4985\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 2933010777.6000 - categorical_accuracy: 0.0000e+00 - top_k_categorical_accuracy: 0.9565 - mean_squared_error: 37.5190\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 1s 62ms/step - loss: 2933332070.4000 - categorical_accuracy: 0.0000e+00 - top_k_categorical_accuracy: 0.9355 - mean_squared_error: 37.5142\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 1s 56ms/step - loss: 2935926873.6000 - categorical_accuracy: 0.0000e+00 - top_k_categorical_accuracy: 0.9810 - mean_squared_error: 37.5004\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 1s 45ms/step - loss: 2934049843.2000 - categorical_accuracy: 0.0000e+00 - top_k_categorical_accuracy: 0.9895 - mean_squared_error: 37.5111\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have shape (26244,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-47424bff20cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m )\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0;31m# Validate and standardize user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     x, _, _ = self._standardize_user_data(\n\u001b[0;32m-> 1486\u001b[0;31m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m         exception_prefix='input')\n\u001b[0m\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0;34m'Error when checking '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                 ' but got array with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    192\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (26244,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Lambda, Layer, Add, Multiply, Masking, Dropout, BatchNormalization\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "import autoencoder as ae\n",
    "\n",
    "\n",
    "num = 162\n",
    "batch_size = 100\n",
    "original_dim = np.power(num, 2)\n",
    "# intermediate_dim = int(original_dim / 3.)\n",
    "output_dim = np.power(num, 2)\n",
    "latent_dim = 20\n",
    "intermediate_dim = 250\n",
    "nb_epoch = 10\n",
    "epsilon_std = 1.0\n",
    "n_packs = 20\n",
    "epochs = 100\n",
    "\n",
    "f = np.load('/data/samples/dicty/' + str(num) + '_org_data.npz')\n",
    "data = f[f.files[0]]\n",
    "fn = '/data/samples/dicty/' + str(num) + '_data.npz'\n",
    "log_dir = '/data/logs/'\n",
    "\n",
    "\n",
    "#np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = original_dim * categorical_crossentropy(x,  x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "#Encoding Layers\n",
    "x = Input(batch_shape=(batch_size, original_dim))\n",
    "h = Dense(intermediate_dim,activation=\"elu\")(x)\n",
    "h= Dropout(0.7)(h)\n",
    "h = Dense(intermediate_dim, activation='elu')(h)\n",
    "h=BatchNormalization(axis=0)(h)\n",
    "h = Dense(intermediate_dim, activation='elu')(h)\n",
    "\n",
    "#Latent layers\n",
    "z_mean=Dense(latent_dim)(h)\n",
    "z_log_var=Dense(latent_dim)(h)\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "#Decoding layers \n",
    "\n",
    "decoder_1= Dense(intermediate_dim, activation='elu')\n",
    "decoder_2=Dense(intermediate_dim, activation='elu')\n",
    "decoder_2d=Dropout(0.7)\n",
    "decoder_3=Dense(intermediate_dim, activation='elu')\n",
    "decoder_out=Dense(output_dim, activation='sigmoid')\n",
    "x_decoded_mean = decoder_out(decoder_3(decoder_2d(decoder_2(decoder_1(z)))))\n",
    "\n",
    "vae = Model(x, x_decoded_mean)\n",
    "\n",
    "#Potentially better results, but requires further hyperparameter tuning\n",
    "#optimizer=keras.optimizers.SGD(lr=0.005, momentum=0.001, decay=0.0, nesterov=False,clipvalue=0.05)\n",
    "vae.compile(optimizer=\"adam\", loss=vae_loss,metrics=[\n",
    "    \"categorical_accuracy\",\n",
    "#     \"fmeasure\",\n",
    "    \"top_k_categorical_accuracy\",\n",
    "    \"mean_squared_error\"\n",
    "])\n",
    "\n",
    "vae.summary()\n",
    "\n",
    "callbacks = [\n",
    "            TensorBoard(log_dir=log_dir, histogram_freq=0, write_graph=True, write_images=True),\n",
    "            EarlyStopping(monitor='loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "        ]\n",
    "\n",
    "vae.fit_generator(\n",
    "    ae.data_generator(fn, n_packs), \n",
    "    steps_per_epoch=n_packs,\n",
    "    callbacks=callbacks,\n",
    "    epochs=epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have shape (26244,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9801832b354f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0;31m# Validate and standardize user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     x, _, _ = self._standardize_user_data(\n\u001b[0;32m-> 1486\u001b[0;31m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m         exception_prefix='input')\n\u001b[0m\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0;34m'Error when checking '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                 ' but got array with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    192\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (26244,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "data.flatten().shape\n",
    "\n",
    "img = vae.predict([data.flatten()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
