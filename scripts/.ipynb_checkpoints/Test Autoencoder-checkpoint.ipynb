{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relationGraph import Relation, RelationGraph, MatrixOfRelationGraph\n",
    "from autoencoder import seedy, AutoEncoder\n",
    "import utilityFunctions as uf\n",
    "from main import test_build_relation_graph_with_symertic_data, test_convert_graph_to_2D_matrix, test_get_matix_for_autoencoder, test_autoencoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from base import load_source\n",
    "from os.path import join\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------RelationGraph-------------\n",
      "Experimental condition\t282\n",
      "1\texpr_T-(282, 1219)\n",
      "1\texpr-(1219, 282)\n",
      "Gene\t1219\n",
      "3\tann-(1219, 116), expr-(1219, 282), ppi-(1219, 1219)\n",
      "2\tann_T-(116, 1219), expr_T-(282, 1219)\n",
      "GO term\t116\n",
      "1\tann_T-(116, 1219)\n",
      "1\tann-(1219, 116)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gene = 'Gene'\n",
    "go_term = 'GO term'\n",
    "exprc = 'Experimental condition'\n",
    "\n",
    "data, rn, cn = load_source(join('dicty', 'dicty.gene_annnotations.csv.gz'))\n",
    "ann = Relation(data=data, x_name=gene, y_name=go_term, name='ann',\n",
    "               x_metadata=rn, y_metadata=cn)\n",
    "\n",
    "data, rn, cn = load_source(join('dicty', 'dicty.gene_expression.csv.gz'))\n",
    "expr = Relation(data=data, x_name=gene, y_name=exprc, name='expr',\n",
    "                x_metadata=rn, y_metadata=cn)\n",
    "expr.matrix = np.log(np.maximum(expr.matrix, np.finfo(np.float).eps))\n",
    "\n",
    "data, rn, cn = load_source(join('dicty', 'dicty.ppi.csv.gz'))\n",
    "ppi = Relation(data=data, x_name=gene, y_name=gene, name='ppi',\n",
    "               x_metadata=rn, y_metadata=cn)\n",
    "\n",
    "ann_t = ann.transpose()\n",
    "expr_t = expr.transpose()\n",
    "\n",
    "relationGraph = RelationGraph()\n",
    "relationGraph.add_relations([ann, expr, ppi, ann_t, expr_t])\n",
    "relationGraph.display_objects()\n",
    "graph = relationGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expr_T (282, 1219)\n",
      "expr (1219, 282)\n",
      "ann (1219, 116)\n",
      "ppi (1219, 1219)\n",
      "ann_T (116, 1219)\n",
      "-------------2D Matrix-------------\n",
      "Objects: GO term: (2, (1501, 1616)), Gene: (1, (282, 1500)), Experimental condition: (0, (0, 281))\n",
      "[0. 1. 0.]\n",
      "[1. 1. 1.]\n",
      "[0. 1. 0.]\n",
      "\n",
      "GO term: 23\n",
      "Gene: 244\n",
      "Experimental condition: 56\n",
      "(323, 323)\n",
      "/data/samples/323_data.npz\n",
      "(323, 323)\n",
      "(162, 162)\n"
     ]
    }
   ],
   "source": [
    "mrg = MatrixOfRelationGraph(graph=graph)\n",
    "mrg.convert_to_2D_matrix()\n",
    "mrg.display_metadata_2D_matrix()\n",
    "data = mrg.density_data(.2)\n",
    "print(data.shape)\n",
    "fn = '/data/samples/' + str(data.shape[0]) + '_data.npz'\n",
    "# fn = '/data/samples/' + str(data.shape[0]) + '_ord_data.npz'    // original data for prediciton\n",
    "print(fn)\n",
    "print(data.shape)\n",
    "\n",
    "f = np.load('/data/samples/162_org_data.npz')\n",
    "data = f[f.files[0]]\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tempfile\n",
    "\n",
    "class my_savez(object):\n",
    "    def __init__(self, file):\n",
    "        # Import is postponed to here since zipfile depends on gzip, an optional\n",
    "        # component of the so-called standard library.\n",
    "        import zipfile\n",
    "        # Import deferred for startup time improvement\n",
    "        import tempfile\n",
    "        import os\n",
    "\n",
    "        if isinstance(file, str):\n",
    "            if not file.endswith('.npz'):\n",
    "                file = file + '.npz'\n",
    "\n",
    "        compression = zipfile.ZIP_STORED\n",
    "\n",
    "        zip = self.zipfile_factory(file, mode=\"w\", compression=compression)\n",
    "\n",
    "        # Stage arrays in a temporary file on disk, before writing to zip.\n",
    "        fd, tmpfile = tempfile.mkstemp(suffix='-numpy.npy')\n",
    "        os.close(fd)\n",
    "\n",
    "        self.tmpfile = tmpfile\n",
    "        self.zip = zip\n",
    "        self.i = 0\n",
    "\n",
    "    def zipfile_factory(self, *args, **kwargs):\n",
    "        import zipfile\n",
    "        import sys\n",
    "        if sys.version_info >= (2, 5):\n",
    "            kwargs['allowZip64'] = True\n",
    "        return zipfile.ZipFile(*args, **kwargs)\n",
    "\n",
    "    def savez(self, *args, **kwds):\n",
    "        import os\n",
    "        import numpy.lib.format as format\n",
    "\n",
    "        namedict = kwds\n",
    "        for val in args:\n",
    "            key = 'arr_%d' % self.i\n",
    "            if key in namedict.keys():\n",
    "                raise ValueError(\"Cannot use un-named variables and keyword %s\" % key)\n",
    "            namedict[key] = val\n",
    "            self.i += 1\n",
    "\n",
    "        try:\n",
    "            for key, val in namedict.items():\n",
    "                fname = key + '.npy'\n",
    "                fid = open(self.tmpfile, 'wb')\n",
    "                try:\n",
    "                    format.write_array(fid, np.asanyarray(val))\n",
    "                    fid.close()\n",
    "                    fid = None\n",
    "                    self.zip.write(self.tmpfile, arcname=fname)\n",
    "                finally:\n",
    "                    if fid:\n",
    "                        fid.close()\n",
    "        finally:\n",
    "            os.remove(self.tmpfile)\n",
    "\n",
    "    def close(self):\n",
    "        self.zip.close()\n",
    "\n",
    "# tmp = '/mag/test.npz'\n",
    "# f = my_savez(tmp)\n",
    "# for i in range(10):\n",
    "#   array = np.zeros(10)\n",
    "#   f.savez(array)\n",
    "# f.close()\n",
    "\n",
    "# # tmp.seek(0)\n",
    "\n",
    "# tmp_read = np.load(tmp)\n",
    "# print (tmp_read.files)\n",
    "# for k, v in tmp_read.iteritems():\n",
    "#      print (k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finnish!!!\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "def mp_worker(arr):\n",
    "#     new_data = uf.sample_generator3(data, num_of_samples=100, density=0.7, 1)\n",
    "    np.random.seed(arr[3])\n",
    "    new_data = uf.sample_generator3(arr[0], num_of_samples=arr[1], density=arr[2])\n",
    "    return new_data\n",
    "\n",
    "\n",
    "def data_generator(data, n_samples, pools, density, filename):\n",
    "    batch_size = 100\n",
    "    p = multiprocessing.Pool(pools)\n",
    "    gen_samples = np.empty((0, data.shape[0] * data.shape[1]))\n",
    "    iterations = int(np.round((n_samples -1)/batch_size)) + 1\n",
    "    \n",
    "    params = [[data, batch_size, density, i] for i in range(iterations)]\n",
    "    \n",
    "    i = 1\n",
    "    f = my_savez(filename)\n",
    "    for result in p.imap(mp_worker, params):\n",
    "        print('samples: ' + str(i * batch_size))\n",
    "        i+=1\n",
    "        f.savez(result)\n",
    "    f.close()\n",
    "    \n",
    "    return gen_samples\n",
    "\n",
    "# fn = '/data/samples/162_data.npz'\n",
    "# d = data_generator(data, 50000, 8, 0.8, fn)\n",
    "# fn = '/data/samples/org_data.npz'\n",
    "# f = my_savez(fn)\n",
    "# f.savez(data)\n",
    "# f.close()\n",
    "\n",
    "print('Finnish!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(162, 162)\n",
      "[[ 5.00661389  4.83041553  4.8573831  ... -0.0227      0.\n",
      "   0.        ]\n",
      " [ 4.86537828  4.71475975  4.96717789 ... -0.0245      0.\n",
      "   0.        ]\n",
      " [ 4.36110939  4.20911566  4.58180944 ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  1.50696212  4.75418484\n",
      "   3.51393065]\n",
      " [ 0.          0.          0.         ... -0.55686956  5.19988833\n",
      "   2.52003245]\n",
      " [ 0.          0.          0.         ...  0.79615493  4.66824827\n",
      "   4.0636104 ]]\n",
      "(162, 162)\n",
      "162\n",
      "/data/weights/12\n",
      "Epoch 1/200\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 4.7013\n",
      "Epoch 2/200\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 1.8918\n",
      "Epoch 3/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 1.3259\n",
      "Epoch 4/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 1.0178\n",
      "Epoch 5/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.8023\n",
      "Epoch 6/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.6473\n",
      "Epoch 7/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.5447\n",
      "Epoch 8/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4715\n",
      "Epoch 9/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.4171\n",
      "Epoch 10/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.3603\n",
      "Epoch 11/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.3173\n",
      "Epoch 12/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.2832\n",
      "Epoch 13/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.2554\n",
      "Epoch 14/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.2320\n",
      "Epoch 15/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.2124\n",
      "Epoch 16/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.1958\n",
      "Epoch 17/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.1814\n",
      "Epoch 18/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.1687\n",
      "Epoch 19/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.1676\n",
      "Epoch 20/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.1487\n",
      "Epoch 21/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.1407\n",
      "Epoch 22/200\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.1340\n",
      "Epoch 23/200\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.1281\n",
      "Epoch 24/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.1249\n",
      "Epoch 25/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.1220\n",
      "Epoch 26/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.1148\n",
      "Epoch 27/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.1116\n",
      "Epoch 28/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.1103\n",
      "Epoch 29/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.1065\n",
      "Epoch 30/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.1056\n",
      "Epoch 31/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.1040\n",
      "Epoch 32/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.1008\n",
      "Epoch 33/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.1026\n",
      "Epoch 34/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0991\n",
      "Epoch 35/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0991\n",
      "Epoch 36/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0970\n",
      "Epoch 37/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0957\n",
      "Epoch 38/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0946\n",
      "Epoch 39/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0939\n",
      "Epoch 40/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0949\n",
      "Epoch 41/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0920\n",
      "Epoch 42/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0911\n",
      "Epoch 43/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0905\n",
      "Epoch 44/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0912\n",
      "Epoch 45/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0908\n",
      "Epoch 46/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0891\n",
      "Epoch 47/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0889\n",
      "Epoch 48/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0882\n",
      "Epoch 49/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0882\n",
      "Epoch 50/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0885\n",
      "Epoch 51/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0867\n",
      "Epoch 52/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0864\n",
      "Epoch 53/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0863\n",
      "Epoch 54/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0857\n",
      "Epoch 55/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0853\n",
      "Epoch 56/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0873\n",
      "Epoch 57/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0852\n",
      "Epoch 58/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0846\n",
      "Epoch 59/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0842\n",
      "Epoch 60/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0838\n",
      "Epoch 61/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0841\n",
      "Epoch 62/200\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.0835\n",
      "Epoch 63/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0837\n",
      "Epoch 64/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0822\n",
      "Epoch 65/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0831\n",
      "Epoch 66/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0832\n",
      "Epoch 67/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0815\n",
      "Epoch 68/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0814\n",
      "Epoch 69/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0819\n",
      "Epoch 70/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0814\n",
      "Epoch 71/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0813\n",
      "Epoch 72/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0806\n",
      "Epoch 73/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0804\n",
      "Epoch 74/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0799\n",
      "Epoch 75/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0801\n",
      "Epoch 76/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0800\n",
      "Epoch 77/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0797\n",
      "Epoch 78/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0791\n",
      "Epoch 79/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0789\n",
      "Epoch 80/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0782\n",
      "Epoch 81/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0783A: 0s - l\n",
      "Epoch 82/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0782\n",
      "Epoch 83/200\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 0.0779\n",
      "Epoch 84/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0775\n",
      "Epoch 85/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0769\n",
      "Epoch 86/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0774\n",
      "Epoch 87/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0769\n",
      "Epoch 88/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0765\n",
      "Epoch 89/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0756\n",
      "Epoch 90/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0759\n",
      "Epoch 91/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0756\n",
      "Epoch 92/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0751\n",
      "Epoch 93/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0750\n",
      "Epoch 94/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0747\n",
      "Epoch 95/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0743\n",
      "Epoch 96/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0741\n",
      "Epoch 97/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0738\n",
      "Epoch 98/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0734\n",
      "Epoch 99/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0732\n",
      "Epoch 100/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0729\n",
      "Epoch 101/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0725\n",
      "Epoch 102/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0722\n",
      "Epoch 103/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0719\n",
      "Epoch 104/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0716\n",
      "Epoch 105/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0713\n",
      "Epoch 106/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0710\n",
      "Epoch 107/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0706\n",
      "Epoch 108/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0703\n",
      "Epoch 109/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0700\n",
      "Epoch 110/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0697\n",
      "Epoch 111/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0694\n",
      "Epoch 112/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0691\n",
      "Epoch 113/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0687\n",
      "Epoch 114/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0684\n",
      "Epoch 115/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0681\n",
      "Epoch 116/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0678\n",
      "Epoch 117/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0675\n",
      "Epoch 118/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0671\n",
      "Epoch 119/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0668\n",
      "Epoch 120/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0665\n",
      "Epoch 121/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0662\n",
      "Epoch 122/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0659\n",
      "Epoch 123/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0656\n",
      "Epoch 124/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0653\n",
      "Epoch 125/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0650\n",
      "Epoch 126/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0647\n",
      "Epoch 127/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0644\n",
      "Epoch 128/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0641\n",
      "Epoch 129/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0638\n",
      "Epoch 130/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0635\n",
      "Epoch 131/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0632\n",
      "Epoch 132/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0630\n",
      "Epoch 133/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0627\n",
      "Epoch 134/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0624\n",
      "Epoch 135/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0621\n",
      "Epoch 136/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0619\n",
      "Epoch 137/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0616\n",
      "Epoch 138/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0613\n",
      "Epoch 139/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0611\n",
      "Epoch 140/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0608\n",
      "Epoch 141/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0605\n",
      "Epoch 142/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0603\n",
      "Epoch 143/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0600\n",
      "Epoch 144/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0598\n",
      "Epoch 145/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0595\n",
      "Epoch 146/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0594\n",
      "Epoch 147/200\n",
      "300/300 [==============================] - 2s 7ms/step - loss: 0.0592\n",
      "Epoch 148/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0586\n",
      "Epoch 149/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0597\n",
      "Epoch 150/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0578\n",
      "Epoch 151/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0591\n",
      "Epoch 152/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0579\n",
      "Epoch 153/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0573\n",
      "Epoch 154/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0581\n",
      "Epoch 155/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0573\n",
      "Epoch 156/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0568\n",
      "Epoch 157/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0573\n",
      "Epoch 158/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0568\n",
      "Epoch 159/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0563\n",
      "Epoch 160/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0566\n",
      "Epoch 161/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0564\n",
      "Epoch 162/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0559\n",
      "Epoch 163/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0559\n",
      "Epoch 164/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0560\n",
      "Epoch 165/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0555\n",
      "Epoch 166/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0554\n",
      "Epoch 167/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0555\n",
      "Epoch 168/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0553\n",
      "Epoch 169/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0550\n",
      "Epoch 170/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0550\n",
      "Epoch 171/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0550\n",
      "Epoch 172/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0549\n",
      "Epoch 173/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0545\n",
      "Epoch 174/200\n",
      "300/300 [==============================] - 1s 4ms/step - loss: 0.0542\n",
      "Epoch 175/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0552\n",
      "Epoch 176/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0551\n",
      "Epoch 177/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0538\n",
      "Epoch 178/200\n",
      "300/300 [==============================] - 2s 5ms/step - loss: 0.0536\n",
      "Epoch 179/200\n",
      "300/300 [==============================] - 1s 5ms/step - loss: 0.0543\n",
      "Epoch 180/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0554\n",
      "Epoch 181/200\n",
      "300/300 [==============================] - 2s 6ms/step - loss: 0.0536\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "(162, 162)\n",
      "MSE: 0.05350979247619257\n"
     ]
    }
   ],
   "source": [
    "fn = '/data/samples/162_org_data.npz'\n",
    "f = np.load(fn)\n",
    "data = f[f.files[0]]\n",
    "print(data.shape)\n",
    "\n",
    "# fn = '/data/samples/162_data.npz'\n",
    "\n",
    "x, y = data.shape\n",
    "# data=data.reshape(1, x * y)\n",
    "ae = AutoEncoder(encoding_dim=20, data=data)\n",
    "ae.encoder_decoder()\n",
    "# ae.fit(batch_size=250, epochs=100)\n",
    "# fn = '/mag/483_data.npz'\n",
    "ae.fit_generator(fn, n_packs=300, epochs=200)\n",
    "ae.save()\n",
    "\n",
    "encoder = ae.load_encoder()\n",
    "decoder = ae.load_decoder()\n",
    "\n",
    "# test_data = np.asarray([data[0].flatten()])\n",
    "# test_data = np.asarray([data.flatten()])\n",
    "test_data = data\n",
    "# print(test_data)\n",
    "# np.random.shuffle(test_data[0])\n",
    "# print(test_data)\n",
    "print(test_data.shape)\n",
    "\n",
    "x = encoder.predict(test_data)\n",
    "y = decoder.predict(x)\n",
    "\n",
    "mse = mean_squared_error(test_data, y)\n",
    "print('MSE: ' + str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_3 (Masking)          (None, 233289)            0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 20)                4665800   \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 233289)            4899069   \n",
      "=================================================================\n",
      "Total params: 9,564,869\n",
      "Trainable params: 9,564,869\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "50/50 [==============================] - 57s 1s/step - loss: nan\n",
      "Epoch 2/200\n",
      " 5/50 [==>...........................] - ETA: 49s - loss: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f4cb93545426>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m         ]\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_packs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_packs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# model.save('/data/sequential/weights/' + str(data.shape[0]) + 'model.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1777\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1778\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1779\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         outs = model.train_on_batch(\n\u001b[0;32m--> 204\u001b[0;31m             x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2912\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2914\u001b[0;31m     \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2915\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2916\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Masking\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "import autoencoder as ae\n",
    "\n",
    "# num = 162   # most densely filled  0.1\n",
    "# num = 323   # most densely filled  0.2\n",
    "num = 483   # most densely filled  0.3\n",
    "# num = 645   # most densely filled  0.4\n",
    "# num = 807   # most densely filled  0.5\n",
    "\n",
    "fn = '/data/samples/' + str(num) + '_org_data.npz'\n",
    "f = np.load(fn)\n",
    "data = f[f.files[0]]\n",
    "\n",
    "fn = '/data/samples/' + str(num) + '_data.npz'\n",
    "        \n",
    "x,y = data.shape\n",
    "data=data.reshape(1, x * y)\n",
    "input_dim = data.shape[1]\n",
    "epochs = 200\n",
    "encoding_dim = 20\n",
    "n_packs = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0, input_shape=(input_dim, )))\n",
    "model.add(Dense(encoding_dim, input_shape=(input_dim, ), activation='relu'))\n",
    "model.add(Dense(input_dim))\n",
    "model.compile(loss='mse', optimizer='sgd')\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Masking(mask_value=0, input_shape=(input_dim, )))\n",
    "# model.add(Dense(int(input_dim / 2), activation='relu'))\n",
    "# model.add(Dense(int(input_dim / 4), activation='relu'))\n",
    "# model.add(Dense(encoding_dim, activation='relu'))\n",
    "# model.add(Dense(int(input_dim / 4), activation='relu'))\n",
    "# model.add(Dense(int(input_dim / 2), activation='relu'))\n",
    "# model.add(Dense(input_dim))\n",
    "# model.compile(loss='mse', optimizer='sgd')\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "log_dir = '/data/logs/'\n",
    "callbacks = [\n",
    "            TensorBoard(log_dir=log_dir, histogram_freq=0, write_graph=True, write_images=True),\n",
    "            EarlyStopping(monitor='loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "        ]\n",
    "\n",
    "model.fit_generator(ae.data_generator(fn, n_packs), steps_per_epoch=n_packs, epochs=epochs, callbacks=callbacks)\n",
    "\n",
    "# model.save('/data/sequential/weights/' + str(data.shape[0]) + 'model.h5')\n",
    "model.save('/data/sequential/weights/' + str(num) + '_model.h5')\n",
    "\n",
    "decoded_imgs = model.predict(data)\n",
    "\n",
    "mse = mean_squared_error(data, decoded_imgs)\n",
    "print('MSE: ' + str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.31138956 4.33326959 4.7106287  ... 4.80063006 2.11492933 4.1626105 ]\n",
      "[4.2677693 4.4455185 4.820116  ... 4.86551   2.086997  4.0896974]\n",
      "\n",
      "(1, 104329)\n",
      "(1, 104329)\n",
      "\n",
      "MSE org data: 0.012063925241763864\n",
      "MSE shuffled data: 45.73822851526672\n",
      "\n",
      "\n",
      "Min org data:-36.04365338911715\n",
      "Max org data:10.068331257647785\n",
      "Mean org data: 1.4092668375654196\n",
      "\n",
      "Min predict:-2.080417\n",
      "Max predict:0.77436584\n",
      "Mean predict: 0.060567502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# num = 162   # most densely filled  0.1\n",
    "num = 323   # most densely filled  0.2\n",
    "# num = 483   # most densely filled  0.3\n",
    "# num = 645   # most densely filled  0.4\n",
    "# num = 807   # most densely filled  0.5\n",
    "\n",
    "f = np.load('/data/samples/' + str(num) + '_org_data.npz')\n",
    "test_data = np.asarray([f[f.files[0]].flatten()])\n",
    "# test_data = np.asarray([data.flatten()])\n",
    "\n",
    "# prediction with normal data\n",
    "model = load_model('/data/sequential/weights/' + str(num) + '_model.h5')\n",
    "y = model.predict(test_data)\n",
    "mse = mean_squared_error(test_data, y)\n",
    "print(test_data[0])\n",
    "print(y[0])\n",
    "print()\n",
    "print(test_data.shape)\n",
    "print(y.shape)\n",
    "print()\n",
    "print('MSE org data: ' + str(mse))\n",
    "\n",
    "\n",
    "# prediction with shuffled data\n",
    "np.random.shuffle(test_data[0])\n",
    "y = model.predict(test_data)\n",
    "mse = mean_squared_error(test_data, y)\n",
    "print('MSE shuffled data: ' + str(mse))\n",
    "print()\n",
    "# print('Mean predict data: ' + str(np.mean(y[0])))\n",
    "# print(test_data[0])\n",
    "# print(y[0])\n",
    "print()\n",
    "print('Min org data:' + str(np.min(test_data)))\n",
    "print('Max org data:' + str(np.max(test_data)))\n",
    "print('Mean org data: ' + str(np.mean(test_data)))\n",
    "print()\n",
    "print('Min predict:' + str(np.min(y)))\n",
    "print('Max predict:' + str(np.max(y)))\n",
    "print('Mean predict: ' + str(np.mean(y)))\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
