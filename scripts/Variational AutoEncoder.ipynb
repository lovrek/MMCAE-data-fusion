{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relationGraph import Relation, RelationGraph, MatrixOfRelationGraph\n",
    "from autoencoder import seedy, AutoEncoder\n",
    "import utilityFunctions as uf\n",
    "from main import test_build_relation_graph_with_symertic_data, test_convert_graph_to_2D_matrix, test_get_matix_for_autoencoder, test_autoencoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from base import load_source\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Layer, Add, Multiply, Masking, Dropout, BatchNormalization\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "import autoencoder as ae\n",
    "\n",
    "\n",
    "# from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (100, 6561)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (100, 250)           1640500     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (100, 250)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (100, 250)           62750       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (100, 250)           400         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (100, 250)           62750       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (100, 20)            5020        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (100, 20)            5020        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (100, 20)            0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (100, 250)           5250        lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (100, 250)           62750       dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (100, 250)           0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (100, 250)           62750       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (100, 6561)          1646811     dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,554,001\n",
      "Trainable params: 3,553,801\n",
      "Non-trainable params: 200\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[100,250] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: dropout_1/cond/dropout/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dropout_1/cond/dropout/Shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: metrics/categorical_accuracy/Mean/_187 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1638_metrics/categorical_accuracy/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-50265c586f7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_packs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1777\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1778\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1779\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         outs = model.train_on_batch(\n\u001b[0;32m--> 204\u001b[0;31m             x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2912\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2914\u001b[0;31m     \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2915\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2916\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[100,250] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: dropout_1/cond/dropout/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dropout_1/cond/dropout/Shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: metrics/categorical_accuracy/Mean/_187 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1638_metrics/categorical_accuracy/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "num = 81\n",
    "num = 162\n",
    "num = 242\n",
    "num = 323\n",
    "# list_of_nums = [81, 162, 242, 323, 403, 483, 564]\n",
    "list_of_nums = [81]\n",
    "# num = 1614\n",
    "batch_size = 100\n",
    "original_dim = np.power(num, 2)\n",
    "# intermediate_dim = int(original_dim / 3.)\n",
    "output_dim = np.power(num, 2)\n",
    "latent_dim = 20\n",
    "intermediate_dim = 250\n",
    "nb_epoch = 10\n",
    "epsilon_std = .1\n",
    "n_packs = 200\n",
    "epochs = 300\n",
    "\n",
    "f = np.load('/data/samples/dicty/' + str(num) + '_org_data.npz')\n",
    "data = f[f.files[0]]\n",
    "fn = '/data/samples/dicty/' + str(num) + '_data.npz'\n",
    "log_dir = '/data/logs/'\n",
    "\n",
    "\n",
    "#np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "def vae_loss2(x, x_decoded_mean):\n",
    "    xent_loss = original_dim * categorical_crossentropy(x,  x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "def vae_loss(input_img, output):\n",
    "    # compute the average MSE error, then scale it up, ie. simply sum on all axes\n",
    "    reconstruction_loss = K.sum(K.square(output - input_img))\n",
    "    # compute the KL loss\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.square(K.exp(z_log_var)), axis=-1)\n",
    "    # return the average loss over all images in batch\n",
    "    total_loss = K.mean(reconstruction_loss + kl_loss)    \n",
    "    return total_loss\n",
    "\n",
    "for num in list_of_nums:\n",
    "    \n",
    "    batch_size = 100\n",
    "    original_dim = np.power(num, 2)\n",
    "    # intermediate_dim = int(original_dim / 3.)\n",
    "    output_dim = np.power(num, 2)\n",
    "    latent_dim = 20\n",
    "    intermediate_dim = 250\n",
    "    nb_epoch = 10\n",
    "    epsilon_std = .1\n",
    "    n_packs = 100\n",
    "    epochs = 300\n",
    "\n",
    "    f = np.load('/data/samples/dicty/' + str(num) + '_org_data.npz')\n",
    "    data = f[f.files[0]]\n",
    "    fn = '/data/samples/dicty/' + str(num) + '_data.npz'\n",
    "    log_dir = '/data/logs/'\n",
    "\n",
    "    #Encoding Layers\n",
    "    x = Input(batch_shape=(batch_size, original_dim))\n",
    "    # x = Masking(mask_value=0, input_shape=(original_dim, ))(x)\n",
    "    h = Dense(intermediate_dim,activation=\"elu\")(x)\n",
    "    h= Dropout(0.7)(h)\n",
    "    h = Dense(intermediate_dim, activation='elu')(h)\n",
    "    h=BatchNormalization(axis=0)(h)\n",
    "    h = Dense(intermediate_dim, activation='elu')(h)\n",
    "\n",
    "    #Latent layers\n",
    "    z_mean=Dense(latent_dim)(h)\n",
    "    z_log_var=Dense(latent_dim)(h)\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    #Decoding layers \n",
    "\n",
    "    decoder_1= Dense(intermediate_dim, activation='elu')\n",
    "    decoder_2=Dense(intermediate_dim, activation='elu')\n",
    "    decoder_2d=Dropout(0.7)\n",
    "    decoder_3=Dense(intermediate_dim, activation='elu')\n",
    "    decoder_out=Dense(output_dim, activation='sigmoid')\n",
    "    x_decoded_mean = decoder_out(decoder_3(decoder_2d(decoder_2(decoder_1(z)))))\n",
    "\n",
    "    vae = Model(x, x_decoded_mean)\n",
    "\n",
    "    #Potentially better results, but requires further hyperparameter tuning\n",
    "    #optimizer=keras.optimizers.SGD(lr=0.005, momentum=0.001, decay=0.0, nesterov=False,clipvalue=0.05)\n",
    "    vae.compile(optimizer=\"adam\", loss=vae_loss,metrics=[\n",
    "        \"categorical_accuracy\",\n",
    "    #     \"fmeasure\",\n",
    "    #     \"top_k_categorical_accuracy\",\n",
    "        \"mean_squared_error\"\n",
    "    ])\n",
    "\n",
    "    vae.summary()\n",
    "\n",
    "    callbacks = [\n",
    "                TensorBoard(log_dir=log_dir, histogram_freq=0, write_graph=True, write_images=True),\n",
    "                EarlyStopping(monitor='loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "            ]\n",
    "\n",
    "    vae.fit_generator(\n",
    "        ae.data_generator([fn], n_packs), \n",
    "        steps_per_epoch=n_packs,\n",
    "        callbacks=callbacks,\n",
    "        epochs=epochs\n",
    "    )\n",
    "    \n",
    "    vae.save('/data/vae/weights/' + str(num) + '_model.h5')\n",
    "    \n",
    "\n",
    "print('Finish!!!')\n",
    "\n",
    "# f = np.load('/data/samples/dicty/' + str(num) + '_data.npz')\n",
    "# data = f[f.files[0]]\n",
    "\n",
    "\n",
    "# data.shape\n",
    "# data.flatten().shape\n",
    "# img = vae.predict(data.reshape(100, original_dim), batch_size=100)\n",
    "\n",
    "# print(np.mean(img))\n",
    "# print(np.mean(data))\n",
    "# mse = mean_squared_error(img, data)\n",
    "# print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------81--------------\n",
      "MSE org data: 0.009575274675055903\n",
      "\n",
      "--------------162--------------\n",
      "MSE org data: 0.009483858867600163\n",
      "\n",
      "--------------242--------------\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor of shape [250,58564] and type float\n\t [[Node: training_11/Adam/zeros_18 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [250,58564] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op 'training_11/Adam/zeros_18', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-b087d4f71c16>\", line 18, in <module>\n    vae = load_model('/data/vae/weights/' + str(num) + '_model.h5',  custom_objects={'vae_loss': vae_loss})\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/saving.py\", line 263, in load_model\n    model._make_train_function()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\", line 598, in _make_train_function\n    params=self._collected_trainable_weights, loss=self.total_loss)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/optimizers.py\", line 471, in get_updates\n    ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/optimizers.py\", line 471, in <listcomp>\n    ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\", line 970, in zeros\n    v = array_ops.zeros(shape=shape, dtype=tf_dtype, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 1551, in zeros\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 2794, in fill\n    \"Fill\", dims=dims, value=value, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor of shape [250,58564] and type float\n\t [[Node: training_11/Adam/zeros_18 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [250,58564] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor of shape [250,58564] and type float\n\t [[Node: training_11/Adam/zeros_18 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [250,58564] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b087d4f71c16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# prediction with normal data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mvae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/data/vae/weights/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_model.h5'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'vae_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvae_loss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    271\u001b[0m         ]\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m           \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_weight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m           logging.warning('Error in loading the saved optimizer '\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/optimizers.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    120\u001b[0m           'of the optimizer (' + str(len(params)) + ')')\n\u001b[1;32m    121\u001b[0m     \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mparam_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m   2658\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2660\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2661\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2662\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    442\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m       \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor of shape [250,58564] and type float\n\t [[Node: training_11/Adam/zeros_18 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [250,58564] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op 'training_11/Adam/zeros_18', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 345, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1312, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 125, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-b087d4f71c16>\", line 18, in <module>\n    vae = load_model('/data/vae/weights/' + str(num) + '_model.h5',  custom_objects={'vae_loss': vae_loss})\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/saving.py\", line 263, in load_model\n    model._make_train_function()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\", line 598, in _make_train_function\n    params=self._collected_trainable_weights, loss=self.total_loss)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/optimizers.py\", line 471, in get_updates\n    ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/optimizers.py\", line 471, in <listcomp>\n    ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\", line 970, in zeros\n    v = array_ops.zeros(shape=shape, dtype=tf_dtype, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 1551, in zeros\n    output = fill(shape, constant(zero, dtype=dtype), name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 2794, in fill\n    \"Fill\", dims=dims, value=value, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor of shape [250,58564] and type float\n\t [[Node: training_11/Adam/zeros_18 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [250,58564] values: [0 0 0]...>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# num = 81\n",
    "list_of_nums = [81, 162, 242, 323, 403, 483, 564]\n",
    "for num in list_of_nums:\n",
    "    print('--------------' + str(num) + '--------------')\n",
    "    original_dim = np.power(num, 2)\n",
    "    fn = '/data/samples/dicty/' + str(num) + '_org_data.npz'\n",
    "    f = np.load(fn)\n",
    "    # test_data = np.asarray([f[f.files[0]].flatten()])\n",
    "    # test_data = np.asarray([data.flatten()])\n",
    "\n",
    "    test_data = f[f.files[0]]\n",
    "\n",
    "    # data.flatten().shape\n",
    "    # test_data = vae.predict(data.reshape(100, original_dim), batch_size=100)\n",
    "\n",
    "    # prediction with normal data\n",
    "\n",
    "    vae = load_model('/data/vae/weights/' + str(num) + '_model.h5',  custom_objects={'vae_loss': vae_loss})\n",
    "\n",
    "    y = vae.predict(test_data.reshape(100, original_dim), batch_size=100)\n",
    "    # y = model.predict(test_data)\n",
    "#     print(test_data.shape)\n",
    "#     print(y.shape)\n",
    "    mse = mean_squared_error(test_data, y)\n",
    "#     print(test_data[0])\n",
    "#     print(y[0])\n",
    "#     print()\n",
    "#     print(test_data.shape)\n",
    "#     print(y.shape)\n",
    "#     print()\n",
    "    print('MSE org data: ' + str(mse))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_37 (InputLayer)           (None, 104329)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_144 (Dense)               (None, 250)          26082500    input_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_145 (Dense)               (None, 20)           5020        dense_144[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_146 (Dense)               (None, 20)           5020        dense_144[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "kl_divergence_layer_14 (KLDiver [(None, 20), (None,  0           dense_145[0][0]                  \n",
      "                                                                 dense_146[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 20)           0           kl_divergence_layer_14[0][1]     \n",
      "__________________________________________________________________________________________________\n",
      "input_38 (InputLayer)           (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_14 (Multiply)          (None, 20)           0           lambda_22[0][0]                  \n",
      "                                                                 input_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 20)           0           kl_divergence_layer_14[0][0]     \n",
      "                                                                 multiply_14[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_14 (Sequential)      (None, 104329)       26191829    add_14[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 52,284,369\n",
      "Trainable params: 52,284,369\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 25s 251ms/step - loss: nan - mean_squared_error: 0.2104\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 21s 211ms/step - loss: nan - mean_squared_error: 0.1930\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 21s 207ms/step - loss: nan - mean_squared_error: 0.1757\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 21s 205ms/step - loss: nan - mean_squared_error: 0.1597\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 22s 216ms/step - loss: nan - mean_squared_error: 0.1452\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 20s 200ms/step - loss: nan - mean_squared_error: 0.1320\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 20s 198ms/step - loss: nan - mean_squared_error: 0.1202\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 20s 199ms/step - loss: nan - mean_squared_error: 0.1097\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 20s 202ms/step - loss: nan - mean_squared_error: 0.1004\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 21s 205ms/step - loss: nan - mean_squared_error: 0.0922\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 20s 200ms/step - loss: nan - mean_squared_error: 0.0849\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 19s 192ms/step - loss: nan - mean_squared_error: 0.0785\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 19s 191ms/step - loss: nan - mean_squared_error: 0.0729\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 20s 202ms/step - loss: nan - mean_squared_error: 0.0680\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 19s 187ms/step - loss: nan - mean_squared_error: 0.0637\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 16s 160ms/step - loss: nan - mean_squared_error: 0.0600\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 16s 155ms/step - loss: nan - mean_squared_error: 0.0568\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 18s 176ms/step - loss: nan - mean_squared_error: 0.0540\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 20s 198ms/step - loss: nan - mean_squared_error: 0.0517\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 19s 187ms/step - loss: nan - mean_squared_error: 0.0497\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 19s 194ms/step - loss: nan - mean_squared_error: 0.0479\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 20s 196ms/step - loss: nan - mean_squared_error: 0.0465\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 19s 186ms/step - loss: nan - mean_squared_error: 0.0452\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 18s 182ms/step - loss: nan - mean_squared_error: 0.0442\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 20s 203ms/step - loss: nan - mean_squared_error: 0.0433\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 19s 191ms/step - loss: nan - mean_squared_error: 0.0426\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 18s 182ms/step - loss: nan - mean_squared_error: 0.0420\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 17s 173ms/step - loss: nan - mean_squared_error: 0.0415\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 19s 192ms/step - loss: nan - mean_squared_error: 0.0410\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 20s 198ms/step - loss: nan - mean_squared_error: 0.0407\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 19s 191ms/step - loss: nan - mean_squared_error: 0.0404\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 19s 187ms/step - loss: nan - mean_squared_error: 0.0401\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 19s 190ms/step - loss: nan - mean_squared_error: 0.0399\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 20s 196ms/step - loss: nan - mean_squared_error: 0.0397\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 19s 188ms/step - loss: nan - mean_squared_error: 0.0396\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 17s 168ms/step - loss: nan - mean_squared_error: 0.0395\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 18s 179ms/step - loss: nan - mean_squared_error: 0.0394\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 20s 201ms/step - loss: nan - mean_squared_error: 0.0393\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 19s 195ms/step - loss: nan - mean_squared_error: 0.0392\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 20s 196ms/step - loss: nan - mean_squared_error: 0.0392\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 20s 202ms/step - loss: nan - mean_squared_error: 0.0391\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - 20s 197ms/step - loss: nan - mean_squared_error: 0.0391\n",
      "Epoch 43/50\n",
      "100/100 [==============================] - 19s 190ms/step - loss: nan - mean_squared_error: 0.0391\n",
      "Epoch 44/50\n",
      "100/100 [==============================] - 20s 200ms/step - loss: nan - mean_squared_error: 0.0390\n",
      "Epoch 45/50\n",
      "100/100 [==============================] - 19s 189ms/step - loss: nan - mean_squared_error: 0.0390\n",
      "Epoch 46/50\n",
      "100/100 [==============================] - 18s 177ms/step - loss: nan - mean_squared_error: 0.0390\n",
      "Epoch 47/50\n",
      "100/100 [==============================] - 19s 185ms/step - loss: nan - mean_squared_error: 0.0390\n",
      "Epoch 48/50\n",
      "100/100 [==============================] - 19s 192ms/step - loss: nan - mean_squared_error: 0.0390\n",
      "Epoch 49/50\n",
      "100/100 [==============================] - 18s 180ms/step - loss: nan - mean_squared_error: 0.0390\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 18s 182ms/step - loss: nan - mean_squared_error: 0.0390\n",
      "0.22505543\n",
      "0.22031216628761538\n",
      "0.038938773334643746\n"
     ]
    }
   ],
   "source": [
    "num = 81    # most densely filled  0.05\n",
    "num = 162   # most densely filled  0.1\n",
    "num = 323   # most densely filled  0.2\n",
    "# num = 483   # most densely filled  0.3\n",
    "# num = 645   # most densely filled  0.4\n",
    "# num = 807   # most densely filled  0.5\n",
    "\n",
    "\n",
    "# original_dim = 784\n",
    "# intermediate_dim = 256\n",
    "original_dim = np.power(num, 2)\n",
    "intermediate_dim = 250\n",
    "latent_dim = 20\n",
    "batch_size = 100\n",
    "epochs = 50\n",
    "epsilon_std = .1\n",
    "n_packs = batch_size\n",
    "\n",
    "f = np.load('/data/samples/dicty/' + str(num) + '_org_data.npz')\n",
    "data = f[f.files[0]]\n",
    "fn = '/data/samples/dicty/' + str(num) + '_data.npz'\n",
    "\n",
    "log_dir = '/data/logs/'\n",
    "\n",
    "\n",
    "def nll(y_true, y_pred):\n",
    "    \"\"\" Negative log likelihood (Bernoulli). \"\"\"\n",
    "\n",
    "    # keras.losses.binary_crossentropy gives the mean\n",
    "    # over the last axis. we require the sum\n",
    "    return K.sum(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "\n",
    "\n",
    "class KLDivergenceLayer(Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        mu, log_var = inputs\n",
    "\n",
    "        kl_batch = - .5 * K.sum(1 + log_var -\n",
    "                                K.square(mu) -\n",
    "                                K.exp(log_var), axis=-1)\n",
    "\n",
    "        self.add_loss(K.mean(kl_batch), inputs=inputs)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "\n",
    "decoder = Sequential([\n",
    "    Dense(intermediate_dim, input_dim=latent_dim, activation='relu'),\n",
    "    Dense(original_dim, activation='sigmoid')\n",
    "])\n",
    "\n",
    "x = Input(shape=(original_dim,))\n",
    "h = Dense(intermediate_dim, activation='relu')(x)\n",
    "\n",
    "z_mu = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "z_mu, z_log_var = KLDivergenceLayer()([z_mu, z_log_var])\n",
    "z_sigma = Lambda(lambda t: K.exp(.5*t))(z_log_var)\n",
    "\n",
    "eps = Input(tensor=K.random_normal(stddev=epsilon_std,\n",
    "                                   shape=(K.shape(x)[0], latent_dim)))\n",
    "z_eps = Multiply()([z_sigma, eps])\n",
    "z = Add()([z_mu, z_eps])\n",
    "\n",
    "x_pred = decoder(z)\n",
    "\n",
    "vae = Model(inputs=[x, eps], outputs=x_pred)\n",
    "vae.compile(optimizer='rmsprop', loss=nll, metrics=[\"mean_squared_error\"])\n",
    "vae.summary()\n",
    "\n",
    "callbacks = [\n",
    "            TensorBoard(log_dir=log_dir, histogram_freq=0, write_graph=True, write_images=True),\n",
    "            EarlyStopping(monitor='loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "        ]\n",
    "\n",
    "vae.fit_generator(\n",
    "    ae.data_generator(fn, n_packs), \n",
    "    steps_per_epoch=n_packs,\n",
    "    callbacks=callbacks,\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "\n",
    "f = np.load(fn)\n",
    "data = f[f.files[0]]\n",
    "\n",
    "\n",
    "data.shape\n",
    "data.flatten().shape\n",
    "img = vae.predict(data.reshape(100, original_dim), batch_size=100)\n",
    "\n",
    "print(np.mean(img))\n",
    "print(np.mean(data))\n",
    "mse = mean_squared_error(img, data)\n",
    "print(mse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
